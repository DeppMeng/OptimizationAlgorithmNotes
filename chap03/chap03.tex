\chapter{Constrained Optimization}
\vspace{1em}
\section{Quadratic Programming}

\begin{align}
    \begin{array}{lll}
        \min \ &Q(\mathbf{x}) &= \frac{1}{2}\mathbf{x}^T G\mathbf{x} + \mathbf{c}^T \mathbf{x} \\
        s.t. \ \ &\mathbf{a}_i^T \mathbf{x} &= b_i, i \in \mathcal{E} = \{1,...,m_e \} \\
        &\mathbf{a}_i^T \mathbf{x} &\geq b_i, i \in \mathcal{I} = \{m_e+1,...,m \}
    \end{array}
\end{align}

We assume that $G$ is a symmetric matrix and $\mathbf{a}_i, i \in \mathcal{E}$
be linearly independent.

\par
\subsection{Solution of Quadratic Programming}
\par
If $G$ be positive semi-definite matrix, the 
Quadratic Programming problem is a convex optimization
problem, so any of its local minima is a global minima.
\par
If $G$ be positive definite matrix, the
solution to the Quadratic Programming problem
is unique, if exists.
\par
If $G$ be indefinite, there is no guarantee to the solution.

\par
\subsection{Equality Constrained Quadratic Programming}
\begin{align}
    \begin{array}{ll}
        \min \ Q(\mathbf{x}) &= \frac{1}{2}\mathbf{x}^T G\mathbf{x} + \mathbf{c}^T \mathbf{x} \\
        s.t. \ \ A \mathbf{x} &= \mathbf{b}
    \end{array}
\end{align}

\par
\subsection{General Quadratic Programming}
\begin{align}
    \begin{array}{lll}
        \min \ &Q(\mathbf{x}) &= \frac{1}{2}\mathbf{x}^T G\mathbf{x} + \mathbf{c}^T \mathbf{x} \\
        s.t. \ \ &\mathbf{a}_i^T \mathbf{x} &= b_i, i \in \mathcal{E} = \{1,...,m_e \} \\
        &\mathbf{a}_i^T \mathbf{x} &\geq b_i, i \in \mathcal{I} = \{m_e+1,...,m \}
    \end{array}
    \label{pro:general-quadratic-programming}
\end{align}

The idea is to remove or transform the inequality constraints.
If the inequality constraint is not active near the solution,
we can ignore the constraint; For the active inequality constraints,
we can use equality constraints to replace them.

\begin{theorem}[Active Set]
    Denote $\mathbf{x}^*$ as a local minima of general quadratic
    problem \emph{(\ref{pro:general-quadratic-programming})}, then $\mathbf{x}^*$ must be a local minima of
    the equality constrained problem
    \begin{align}
        (\textnormal{EQ})\left\{
            \begin{array}{ll}
                \min \ Q(\mathbf{x}) &= \frac{1}{2} \mathbf{x}^T G\mathbf{x}
                + \mathbf{c}^T\mathbf{x} \\
                s.t. \ \ \mathbf{a}_i^T\mathbf{x} &= b_i, i \in \mathcal{E}
                \cup \mathcal{I}(\mathbf{x}^*)
            \end{array}
        \right.
    \end{align}
    Meanwhile, if $\mathbf{x}^*$ is a feasible point of \emph{(\ref{pro:general-quadratic-programming})},
    and the K-T point of (EQ), $\lambda^* \geq 0, i \in \mathcal{I}(\mathbf{x}^*)$,
    then $\mathbf{x}^*$ must be the K-T point of \emph{(\ref{pro:general-quadratic-programming})}.
\end{theorem}
\begin{proof}
    Recall the K-T condition, we can get that
    there exists $\lambda_i \geq 0, i \in I(\mathbf{x}^*)$ and $\mu_j$ s.t.
    \begin{align}
        \bigtriangledown Q(\mathbf{x}^*) - \sum_{i\in I(\mathbf{x}^*)} \lambda_i \mathbf{a}_i
        - \sum_{j \in \mathcal{E}} \mu_j \mathbf{a}_j = 0
        \label{equ:qp1}
    \end{align}
    the K-T condition of (EQ) is there exists $\lambda_i, i \in \mathcal{E} \cup \mathcal{I}(\mathbf{x}^*)$,
    s.t.
    \begin{align}
        \bigtriangledown Q(\mathbf{x}^*)
        - \sum_{j \in \mathcal{E} \cup \mathcal{I}(\mathbf{x}^*)} \lambda_j \mathbf{a}_j = 0
        \label{equ:qp2}
    \end{align}
    Appearently If $\mathbf{x}^*$ satisfies (\ref{equ:qp1}), then it also satisfies (\ref{equ:qp2}).
    On the other hand, if $\mathbf{x}^*$ satisfies (\ref{equ:qp2}) and $\lambda_i \geq 0, i \in I(\mathbf{x}^*)$,
    we have
    \begin{align}
        &\bigtriangledown Q(\mathbf{x}^*)
        - \sum_{j \in \mathcal{E} \cup \mathcal{I}(\mathbf{x}^*)} \lambda_j \mathbf{a}_j = 0 \\
        \Leftrightarrow \ &
        \bigtriangledown Q(\mathbf{x}^*) - \sum_{i\in I(\mathbf{x}^*)} \lambda_i \mathbf{a}_i
        - \sum_{j \in \mathcal{E}} \lambda_j \mathbf{a}_j = 0
    \end{align}
    i.e., $\mathbf{x}^*$ satisfies (\ref{equ:qp1}).
    
\end{proof}

% \section{Non-linear Constrained Optimization}
\section{Equality Constrained Problem}
\par
\subsection{Lagrange-Newton method}
\par
\begin{align}
    \min \ &f(\mathbf{x}) \\
    s.t. \ &\mathbf{c}(\mathbf{x}) = \mathbf{0}
\end{align}
where $\mathbf{c}(\mathbf{x}) = (c_1(\mathbf{x}),..., c_m(\mathbf{x}))^T$.
\par
Denote $A(\mathbf{x}) = [\bigtriangledown\mathbf{c}(\mathbf{x})]^T 
= (\bigtriangledown c_1(\mathbf{x}),..., \bigtriangledown c_m(\mathbf{x}))^T$.
The K-T condition of the problem is there exists
$\mathbf{\lambda} \in \mathbb{R}^m$ s.t.
\begin{align}
    \bigtriangledown f(\mathbf{x}) - A(\mathbf{x})^T \mathbf{\lambda} = \mathbf{0}
\end{align}
and $\mathbf{c}(\mathbf{x}) = \mathbf{0}$.
\par
We can use Newton-Raphson method to solve the equations by
\begin{align}
    \left(\begin{array}{ll}
        W(\mathbf{x}, \mathbf{\lambda}) & -A(\mathbf{x})^T \\
        -A(\mathbf{x}) & 0
    \end{array}\right)
    \left(\begin{array}{ll}
        \delta_{x} \\
        \delta_\lambda
    \end{array}\right) = -
    \left(
        \begin{array}{ll}
            \bigtriangledown f(\mathbf{x}) - A(\mathbf{x})^T \mathbf{\lambda} \\
            \mathbf{c}(\mathbf{x})
        \end{array}\right)
        \label{equ:lagrange-newton1}
\end{align}
where $W(\mathbf{x}, \mathbf{\lambda}) = \bigtriangledown^2 f(\mathbf{x})
- \sum_{i=1}^m \lambda_i \bigtriangledown^2 c_i(\mathbf{x})$.
\par
We called the method above as \emph{Lagrange-Newton Method}.
\par
Here we can define 
\begin{align}
    \psi(\mathbf{x}, \lambda) = \parallel \bigtriangledown f(\mathbf{x})
    - A(\mathbf{x})^T\lambda \parallel^2 + \parallel\mathbf{c}(\mathbf{x})
    \parallel^2
\end{align}
so that $\psi$ is a descent function to Lagrange-Newton method.
\begin{align}
    \bigtriangledown \psi(\mathbf{x}, \lambda)^T
    \left(\begin{array}{ll}
        \delta_{x} \\
        \delta_\lambda
    \end{array}\right) = 
    -2 \psi(\mathbf{x}, \lambda) \neq 0
\end{align}
\par
\subsection{Sequential Quadratic Programming method}
\par
(\ref{equ:lagrange-newton1}) can be rewritten into
\begin{align}
    \left\{
        \begin{array}{ll}
            W(\mathbf{x}, \lambda) \delta_x + \bigtriangledown f(\mathbf{x})
            &= A(\mathbf{x})^T(\lambda + \delta_\lambda) \\
            \mathbf{c}(\mathbf{x}) + A(\mathbf{x})\delta_x &= \mathbf{0}
        \end{array}
        \right.
\end{align}
From K-T condition, we notice that $\delta_x$ is the
K-T point of the following Quadratic Programming problem
\begin{align}
    \begin{array}{ll}
        \min \ &\frac{1}{2} \mathbf{d}^T W(\mathbf{x}, \lambda) \mathbf{d}
        + \bigtriangledown f(\mathbf{x})^T\mathbf{d} \\
        s.t. \ &\mathbf{c}(\mathbf{x}) + A(\mathbf{x})\mathbf{d} = 0
    \end{array}
\end{align}

So we can solve a Quadratic Programming subproblem to
derive $\delta_x$, we called this method
\emph{Sequential Quadratic Programming}.

\par
\section{General Nonlinear Constrained Problem}
\par
\subsection{Sequential Quadratic Programming method}
\par
\begin{align}
    \begin{array}{lll}
        \min \ &f(\mathbf{x}) \\
        s.t. \ &c_i(\mathbf{x}) = 0, \quad i \in \mathcal{E} = \{1,...,m_e\} \\
        &c_i(\mathbf{x}) \geq 0, \quad i \in \mathcal{I} = \{m_e+1,...,m\}
    \end{array}
    \label{pro:sqp-original}
\end{align}
Similarly, we can construct subproblem
\begin{align}
    \begin{array}{lll}
        \min \ &\frac{1}{2}\mathbf{d}^TW\mathbf{d} + \mathbf{g}^T\mathbf{d} \\
        s.t. \ &c_i(\mathbf{x}) + \mathbf{a}_i(\mathbf{x})^T\mathbf{d} = 0, i \in \mathcal{E} \\
         &c_i(\mathbf{x}) + \mathbf{a}_i(\mathbf{x})^T\mathbf{d} \geq 0, i \in \mathcal{I}
    \end{array}
    \label{pro:sqp}
\end{align}
Here, $W$ is the Hesse matrix (or its approximation) of the Lagrange
function of (\ref{pro:sqp-original}), $\mathbf{g} = \bigtriangledown f(\mathbf{x})$,
$A(\mathbf{x}) = (\mathbf{a}_1(\mathbf{x}),...,\mathbf{a}_m(\mathbf{x})$.

Denote the solution to subproblem (\ref{pro:sqp}) as $\mathbf{d}$,
the corresponding Lagrange multiplier vector $\bar{\lambda}$,
so we have
\begin{align}
    \left\{
        \begin{array}{llll}
            &W\mathbf{d} + \mathbf{g} = A(\mathbf{x})^T\bar{\lambda} \\
            &\bar{\lambda}_i \geq 0, i \in \mathcal{I} \\
            &\mathbf{c}(\mathbf{x}) + A(\mathbf{x})\mathbf{d} = 0, i \in \mathcal{E} \\
            &\mathbf{c}(\mathbf{x}) + A(\mathbf{x})\mathbf{d} \geq 0, i \in \mathcal{I}
        \end{array}\right.
\end{align}

\par
\subsection{Penalty method}
\par
For nonlinear constrained porblem (\ref{pro:sqp-original}),
we can use objective function $f(\mathbf{x})$
and constraint function $\mathbf{c}(\mathbf{x})$
to construct \emph{Penalty function}
\begin{align}
    P(\mathbf{x}) = P(f(\mathbf{x}), \mathbf{c}(\mathbf{x}))
\end{align}

We need the penalty function have the property that:
for feasible points, $P(\mathbf{x}) = f(\mathbf{x})$,
otherwise, $P(\mathbf{x}) > f(\mathbf{x})$.
\par
To measure the destructiveness to the constraints,
we define $\mathbf{c}(\mathbf{x})\_$
\begin{align}
    \left\{
        \begin{array}{ll}
            c_i(\mathbf{x})\_ &= c_i(\mathbf{x}), \quad \quad \quad \quad \quad i\in \mathcal{E} \\
            c_i(\mathbf{x})\_ &= |\min \{ 0, c_i(\mathbf{x}) \}|,\ \ i \in \mathcal{I}
        \end{array}\right.
\end{align}

Consider simple penalty function
\begin{align}
    P_\sigma(\mathbf{x}) = f(\mathbf{x}) + 
    \sigma \parallel \mathbf{c}(\mathbf{x})\_ \parallel^2
\end{align}
Denote $\mathbf{x}(\sigma)$ as the solution to unconstrained
problem $\min P_\sigma(\mathbf{x})$, we have the following lemma:

\begin{lemma}[Penalty method]
    If $\mathbf{x}(\sigma)$ is a feasible point of nonlinear
    constrained problem \textnormal{(\ref{pro:sqp-original})}, then $\mathbf{x}(\sigma)$
    aslo is the solution to \textnormal{(\ref{pro:sqp-original})}.
\end{lemma}

\begin{proof}
    From the definition of penalty function,
    we have $P(\mathbf{x}) = f(\mathbf{x})$,
    $\mathbf{x} \in \mathcal{S}$.
    If $\mathbf{x}(\sigma)$ is the solution to 
    $\min P(\mathbf{x})$, i.e.,
    \begin{align}
        P(\mathbf{x}(\sigma)) &\leq P(\mathbf{x}_0), \ \forall
        \mathbf{x}_0 \in \mathbb{R}^{n} \\
        f(\mathbf{x}(\sigma)) &\leq f(\mathbf{x}_0), \ \forall
        \mathbf{x}_0 \in \mathcal{S}
    \end{align}
that is, $\mathbf{x}(\sigma)$ is the solution to (\ref{pro:sqp-original}).

\end{proof}

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwInOut{Return}{return}
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n$, $\sigma_0 > 0$, $\beta > 1$,
     $\epsilon > 0$, $k:= 0$\;
     \While{$\parallel \mathbf{c}(\mathbf{x}(\sigma_{k-1}))\_ \parallel \geq \epsilon$}{
        solve the subproblem $\min_{\mathbf{x}\in \mathbb{R}^n} P_{\sigma_k}(\mathbf{x})$
        to get the solution $\mathbf{x}(\sigma_k)$\;
        $\mathbf{x}^{(k+1)} = \mathbf{x}(\sigma_k)$,
        $\sigma_{k+1} = \beta \sigma_k$\;
        $k:=k+1$\;
     }
     \Return{$\mathbf{x}(\sigma_{k-1})$}
     \caption{Penalty Method Algorithm}
\end{algorithm}

\begin{theorem}[Convergence of Penalty method]
    If $\epsilon > \min_{\mathbf{x} \in \mathbb{R}^n}
    \parallel \mathbf{c}(\mathbf{x})\_ \parallel$,
    then the algorithm can terminate in finite steps.
\end{theorem}

\begin{lemma}
    Let $\sigma_{k+1} > \sigma_k > 0$, then we have
    $P_{\sigma_k}(\mathbf{x}(\sigma_k)) \leq P_{\sigma_{k+1}}(\mathbf{x}(\sigma_{k+1}))$,
    $\parallel \mathbf{c}(\mathbf{x}(\sigma_k))\_ \parallel
    \geq \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ \parallel$,
    $f(\mathbf{x}(\sigma_k)) \leq f(\mathbf{x}(\sigma_{k+1}))$.
    \label{lemma:penalty1}
\end{lemma}
\begin{proof}
    \begin{align}
        P_{\sigma_{k+1}}(\mathbf{x}(\sigma_{k+1}))
        &= f(\mathbf{x}(\sigma_{k+1})) + 
        \sigma_{k+1} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ \parallel^2 \\
        &\geq f(\mathbf{x}(\sigma_{k+1})) + 
        \sigma_{k} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ \parallel^2 \\
        &\geq \min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x}) + 
        \sigma_{k} \parallel \mathbf{c}(\mathbf{x})\_ \parallel^2 \\
        &= P_{\sigma_k}(\mathbf{x}(\sigma_k))
    \end{align}
    From the definition, we have
    \begin{align}
        &f(\mathbf{x}(\sigma_{k})) + 
        \sigma_{k+1} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2 \\
        \geq 
        &f(\mathbf{x}(\sigma_{k+1})) + 
        \sigma_{k+1} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2 \\
        \geq 
        &f(\mathbf{x}(\sigma_{k+1})) + 
        \sigma_{k} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2 \\
        \geq 
        &f(\mathbf{x}(\sigma_{k})) + 
        \sigma_{k} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2
    \end{align}
    From the inequalities above, we have
    \begin{align}
        &\sigma_{k} (\parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2 - \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2) \\
        \leq
        &f(\mathbf{x}(\sigma_{k+1})) - f(\mathbf{x}(\sigma_{k})) \\
        \leq 
        & \sigma_{k+1} (\parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2 - \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2)
    \end{align}
    So that
    \begin{align}
        \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel \geq \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel
    \end{align}
    Then
    \begin{align}
        0\leq \sigma_{k} (\parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2 - \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2)
        \leq
        f(\mathbf{x}(\sigma_{k+1})) - f(\mathbf{x}(\sigma_{k}))
    \end{align}
    i.e.,
    \begin{align}
        f(\mathbf{x}(\sigma_{k+1})) \geq f(\mathbf{x}(\sigma_{k}))
    \end{align}
    
\end{proof}

\begin{lemma}
    Denote $\bar{\mathbf{x}}$ as the solution to
    problem \emph{(\ref{pro:sqp-original})}, then for all
    $\sigma_k>0$,
    \begin{align}
        f(\bar{\mathbf{x}}) \geq
        P_{\sigma_k}(\mathbf{x}(\sigma_k))
        \geq f(\mathbf{x}(\sigma_k))
    \end{align}
\end{lemma}
\begin{proof}
    For all $\sigma_k>0$,
    \begin{align}
        f(\bar{\mathbf{x}}) &= 
        \min_{\mathbf{x} \in \mathbb{R}^n}
        \lim_{\sigma \rightarrow \infty}
        f(\mathbf{x}) + \sigma \parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2 \\
        &\geq \min_{\mathbf{x} \in \mathbb{R}^n}
        f(\mathbf{x}) + \sigma_k \parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2\\
        &= f(\mathbf{x}(\sigma_k)) + \sigma_k \parallel \mathbf{c}
        (\mathbf{x}(\sigma_{k}))\_ \parallel^2 \\
        &\geq f(\mathbf{x}(\sigma_k))
    \end{align}
    
\end{proof}
\begin{lemma}
    Let $\delta = \parallel \mathbf{c}
    (\mathbf{x}(\sigma))\_ \parallel$,
    then $\mathbf{x}(\sigma)$ is also
    the solution to the problem
    \begin{align}
        \begin{array}{ll}
            \min \ &f(\mathbf{x}) \\
            s.t. \ &\parallel \mathbf{c}
            (\mathbf{x})\_ \parallel \leq \delta
        \end{array}
    \end{align}
\end{lemma}
\begin{proof}
    The problem is equivalent to
    \begin{align}
        \begin{array}{ll}
            \min \ &f(\mathbf{x}) \\
            s.t. \ &\parallel \mathbf{c}
            (\mathbf{x})\_ \parallel \leq
            \parallel \mathbf{c}
            (\mathbf{x}(\sigma))\_ \parallel
        \end{array}
    \end{align}
    \begin{align}
        f(\mathbf{x}(\sigma)) + \sigma \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel^2 = 
        \min_{\mathbf{x} \in \mathbb{R}^n}
        f(\mathbf{x}) + \sigma \parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2
    \end{align}
    Then for all $\mathbf{x} \in \mathbb{R}^n$, we have
    \begin{align}
        f(\mathbf{x}(\sigma)) + \sigma \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel^2 &\leq 
        f(\mathbf{x}) + \sigma \parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2 \\
        f(\mathbf{x}(\sigma)) - f(\mathbf{x}) &\leq
        \sigma(\parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2 - \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel^2)
    \end{align}
    That is, if $\parallel \mathbf{c}
    (\mathbf{x})\_ \parallel \leq \parallel \mathbf{c}
    (\mathbf{x}(\sigma))\_ \parallel$, then
    \begin{align}
        f(\mathbf{x}(\sigma)) - f(\mathbf{x}) \leq
        \sigma(\parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2 - \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel^2) \leq 0
    \end{align}
    i.e., for all $\mathbf{x} \in \mathbb{R}^n$,
    $f(\mathbf{x}(\sigma)) \leq f(\mathbf{x})$.
    
\end{proof}
\subsection{Argumented Lagrange function method}
\subsubsection{Revisit Penalty method}
Consider equality constrained problem
\begin{align}
    \begin{array}{ll}
        \min \ &f(\mathbf{x}) \\
        s.t. \ &\mathbf{c}(\mathbf{x}) = 0
    \end{array}
    \label{pro:penalty1}
\end{align}
The Lagrange function of (\ref{pro:penalty1}) is
\begin{align}
    \mathcal{L}(\mathbf{x}, \lambda) = 
    f(\mathbf{x}) - \lambda^T \mathbf{c}(\mathbf{x})
\end{align}
From K-T condition, we have for global optimal point
$\mathbf{x}^*$,
\begin{align}
    \left\{
        \begin{array}{ll}
            \bigtriangledown_{\mathbf{x}} \mathcal{L}
            (\mathbf{x}^*, \lambda^*) = 0 \\
            \bigtriangledown_{\lambda} \mathcal{L}
            (\mathbf{x}^*, \lambda^*) = 0
        \end{array}
        \right.
\end{align}
i.e., $\mathbf{x}^*$ is a stable point of
$\mathcal{L}(\mathbf{x}, \lambda)$.
Notice that
\begin{align}
    \bigtriangledown{\mathbf{x}} \mathcal{L}
    (\mathbf{x}^*, \lambda^*) = 
    \bigtriangledown f(\mathbf{x}^*) - 
    \sum_i \lambda_i^* \bigtriangledown c_i(\mathbf{x}^*)
    \label{equ:penalty1}
\end{align}
For the corresponding penalty function
\begin{align}
    P_\sigma (\mathbf{x}) =
    f(\mathbf{x}) + \sigma \parallel
    \mathbf{c}(\mathbf{x}) \parallel^2
\end{align}
we have the K-T condition is
\begin{align}
    \bigtriangledown P_\sigma (\mathbf{x}^*) &= 
    \bigtriangledown f(\mathbf{x}^*) + 
    2\sigma \mathbf{c}^T(\mathbf{x}^*) 
    \bigtriangledown \mathbf{c}(\mathbf{x}^*) \\
    &= 
    \bigtriangledown f(\mathbf{x}^*) + 
    \sum_i 2\sigma c_i(\mathbf{x}^*) 
    \bigtriangledown c_i(\mathbf{x}^*) = 0
    \label{equ:penalty2}
\end{align}

If we want (\ref{equ:penalty2}) to be a good
approximation of (\ref{equ:penalty1}), i.e.,
\begin{align}
    \lambda_i^* \approx - 2 \sigma c_i(\mathbf{x}^*)
\end{align}

Notice that $c_i(\mathbf{x}^*) \approx 0$, so we need
$|\sigma| \rightarrow \infty $.

\subsubsection{Argumented Lagrange function method}
Consider \emph{Argumented Lagrange function}
\begin{align}
    \min_{\mathbf{x}, \lambda}
     \ P(\mathbf{x}, \lambda, \sigma) = 
    \mathcal{L}(\mathbf{x}, \lambda) + 
    \frac{\sigma}{2} \parallel \mathbf{c}
    (\mathbf{x}) \parallel^2
    \label{pro:argu_lag1}
\end{align}
The K-T condition of the function is
\begin{align}
    \left\{
        \begin{array}{ll}
            \bigtriangledown_{\mathbf{x}} P
            (\mathbf{x}^*, \lambda^*, \sigma) = 0 \\
            \bigtriangledown_{\lambda} P
            (\mathbf{x}^*, \lambda^*, \sigma) = 0
        \end{array}
        \right.
\end{align}
\begin{align}
    \bigtriangledown_{\lambda} P
    (\mathbf{x}^*, \lambda^*, \sigma) = 
    \mathbf{c}(\mathbf{x}) = 0
\end{align}
\begin{align}
    \bigtriangledown_{\mathbf{x}} P(\mathbf{x}^*, \lambda^*, \sigma) &= 
    \bigtriangledown f(\mathbf{x}^*) - 
    \sum_i (\lambda_i^* - \sigma c_i(\mathbf{x}^*)) \bigtriangledown
    c_i(\mathbf{x}^*) \\ &=
    \bigtriangledown f(\mathbf{x}^*) - 
    \sum_i \lambda_i^* \bigtriangledown
    c_i(\mathbf{x}^*) = 0
\end{align}
i.e., the K-T condition of $P$ is similar to the original
problem (\ref{pro:penalty1}).
\begin{theorem}
    Suppose $\mathbf{x}^*$ and $\lambda^*$ satisfy
    the K-T condition of (\ref{pro:penalty1}),
    then there exists $\bar{\sigma}$ such that
    when $\sigma > \bar{\sigma}$,
    $\mathbf{x}^*$ is the strict local minima
    of $P(\mathbf{x}, \lambda^*, \sigma)$.
\end{theorem}
\begin{proof}
    Appearently if $\mathbf{x}^*$ and $\lambda^*$ satisfy
    the K-T condition of (\ref{pro:penalty1}),
    then $\mathbf{x}^*$ and $\lambda^*$ also satisfy
    the K-T condition of (\ref{pro:argu_lag1}).
    \par
    For (\ref{pro:argu_lag1}), we can always find
    $\bar{\sigma}$ when $\sigma > \bar{\sigma}$,
    the problem is convex. In this case, the
    K-T condition is sufficient and necessary
    condition of optimal points.
\end{proof}

However, the optimal value $\lambda^*$ remains
unknown.

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwInOut{Return}{return}
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n$, $\sigma_0 > 0$, $\alpha > 1$,
     $0 < \beta < 1$, $\epsilon > 0$, $k:= 0$\;
     \While{$\parallel \mathbf{c}(\mathbf{x}^{(k)}\parallel \geq \epsilon$}{
        $\mathbf{x}^{(k+1)} = 
        \arg\min_{\mathbf{x}\in \mathbb{R}^n}
        P(\mathbf{x}, \lambda^{(k)}, \sigma)$\;
        $\lambda^{(k+1)} = \lambda^{(k)} - 
        \sigma \mathbf{c}(\mathbf{x}^{(k+1)})$\;
        \If{$\parallel \mathbf{c}(\mathbf{x}^{(k+1)}\parallel / 
        \parallel \mathbf{c}(\mathbf{x}^{(k)}\parallel
        \geq \beta$}{$\sigma := \alpha \sigma$}
        $k:=k+1$\;
     }
     \Return{$\mathbf{x}^{(k)}$}
     \caption{Argumented Lagrange Algorithm}
\end{algorithm}


\subsection{Barrier method}

\begin{align}
    \begin{array}{ll}
        \min \ &f(\mathbf{x}) \\
        s.t. \ &g_i (\mathbf{x}) \geq 0, i = 1,..., m
    \end{array}
    \label{pro:barrier1}
\end{align}

We use $\mathbf{int}S$ to denote the interior
of feasible region, where
$S = \{ \mathbf{x} \ | \ g_i(\mathbf{x}) \geq 0,
i = 1,...,m \}$. Define \emph{Barrier function}
\begin{align}
    B(\mathbf{x}, \theta) = f(\mathbf{x}) + 
    \theta \psi (\mathbf{x})
\end{align}
Where barrier factor $\theta$ is a small positive number,
$\psi(\mathbf{x})$ is a continuous function.
When $\mathbf{x} \rightarrow \partial S$,
$\psi(\mathbf{x}) \rightarrow + \infty$.
We can derive the approximate solution to
the original problem (\ref{pro:barrier1})
\begin{align}
    \begin{array}{ll}
        \min \ &B(\mathbf{x}, \theta) \\
        s.t. \ &\mathbf{x} \in \mathbf{int}S
    \end{array}
\end{align}

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwInOut{Return}{return}
    \KwData{Cost function $f$, feasible region $S$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbf{int}S$, $\theta_0 > 0$,
     $0 < \beta < 1$, $\epsilon > 0$, $k:= 0$\;
     \While{$\theta_k \psi(\mathbf{x}^{(k)} \geq \epsilon$}{
        $\mathbf{x}^{(k+1)} = 
        \arg\min_{\mathbf{x}\in \mathbf{int}S}
        f(\mathbf{x}) + \theta_k \psi(\mathbf{x})$\;
        $\theta_{k+1} := \beta \theta_{k}$\;
        $k:=k+1$\;
     }
     \Return{$\mathbf{x}^{(k)}$}
     \caption{Barrier Algorithm}
\end{algorithm}

\begin{theorem}
    Suppose $\theta_k > \theta_{k+1} > 0$,
    denote $\mathbf{x}(\theta) = \arg\min_{\mathbf{x}}
    B(\mathbf{x}, \theta)$, then
    \begin{align}
        B(\mathbf{x}(\theta_k), \theta_k)
        &\geq B(\mathbf{x}(\theta_{k+1}), \theta_{k+1}) \\
        \psi(\mathbf{x}(\theta_k)) &\leq
        \psi(\mathbf{x}(\theta_{k+1})) \\
        f(\mathbf{x}(\theta_k)) &\geq
        f(\mathbf{x}(\theta_{k+1}))
    \end{align}
\end{theorem}
\begin{proof}
    Similar to Proof of Lemma (\ref{lemma:penalty1}),
    \begin{align}
        B(\mathbf{x}(\theta_k), \theta_k) &= 
        f(\mathbf{x}(\theta_k))
        + \theta_k \psi(\mathbf{x}(\theta_k)) \\
        &\geq f(\mathbf{x}(\theta_k))
        + \theta_{k+1} \psi(\mathbf{x}(\theta_k)) \\
        &\geq \min_{\mathbf{x}\in \mathbf{int}S}
        f(\mathbf{x})
        + \theta_{k+1} \psi(\mathbf{x}) \\
        &= B(\mathbf{x}(\theta_{k+1}), \theta_{k+1})
    \end{align}
    From
    \begin{align}
        &f(\mathbf{x}(\theta_{k+1}))
        + \theta_k \psi(\mathbf{x}(\theta_{k+1})) \\
        \geq &f(\mathbf{x}(\theta_k))
        + \theta_k \psi(\mathbf{x}(\theta_k)) \\
        \geq &f(\mathbf{x}(\theta_k))
        + \theta_{k+1} \psi(\mathbf{x}(\theta_k)) \\
        \geq &f(\mathbf{x}(\theta_{k+1}))
        + \theta_{k+1} \psi(\mathbf{x}(\theta_{k+1}))
    \end{align}
    we have
    \begin{align}
        \theta_{k}(\psi(\mathbf{x}(\theta_{k})) - 
        \psi(\mathbf{x}(\theta_{k+1}))) \leq
        f(\mathbf{x}(\theta_{k+1})) - 
        f(\mathbf{x}(\theta_{k})) \leq
        \theta_{k+1}(\psi(\mathbf{x}(\theta_{k})) - 
        \psi(\mathbf{x}(\theta_{k+1})))
    \end{align}
    notice that $\theta_k > \theta_{k+1} > 0$, so
    \begin{align}
        \psi(\mathbf{x}(\theta_k)) \leq
        \psi(\mathbf{x}(\theta_{k+1}))
    \end{align}
    \begin{align}
        f(\mathbf{x}(\theta_{k+1})) - 
        f(\mathbf{x}(\theta_{k})) &\leq
        \theta_{k+1}(\psi(\mathbf{x}(\theta_{k})) - 
        \psi(\mathbf{x}(\theta_{k+1}))) \leq 0 \\
        f(\mathbf{x}(\theta_{k+1})) &\leq 
        f(\mathbf{x}(\theta_{k})) 
    \end{align}
\end{proof}
\chapter{Unconstrained Optimization}
\vspace{1em}
\section{Gradient Based Methods}

\begin{align}
    \min_{x \in \mathbb{R}^n} f(x)
\end{align}
\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Solution set $\Omega$, cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, k:= 0$\;
     \While{$x^{(k)} \notin \Omega$}{
      $d^{(k)} = - H_k \bigtriangledown f(x^{(k)})$,
        ($H_k$ is a positive definite symmetrical matrix)\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$, $k:= k+1$
     }
     \caption{Example of gradient based algorithm}
\end{algorithm}
\subsection{Determine Search Direction}
\subsubsection{First-order gradient method}
\par
For unconstrained optimization problem 
\begin{align}
    \min_{x \in \mathbb{R}^n} f(x)
\end{align}
We have
\begin{align}
    f(x) = f(x^{(k)}) + \bigtriangledown f(x^{(k)})^T(x - x^{(k)})
    + O(\parallel x - x^{(k)} \parallel^2)
\end{align}
Set $d^{(k)} = - \bigtriangledown f(x^{(k)})$,
when $\alpha_k$ is sufficiently small,
\begin{align}
    f(x^{(k)} + \alpha_k d^{(k)}) < f(x^{(k)})
\end{align}

\subsubsection{Second-order gradient method -- Newton Direction}
\par
\begin{align}
    f(x) = &f(x^{(k)}) + \bigtriangledown f(x^{(k)})^T(x - x^{(k)}) \\
    &+ \frac{1}{2} (x - x^{(k)})^T \bigtriangledown^2 f(x^{(k)}) (x - x^{(k)})
    + O(\parallel x - x^{(k)} \parallel^3)
\end{align}
Set $d^{(k)} = -G_k^{-1} \bigtriangledown f(x^{(k)})$,
 where $G_k = \bigtriangledown^2 f(x^{(k)})$,
 i.e., Hesse matrix of $f$ at $x^{(k)}$.
\subsection{Determine Step Factor -- Line Search}

\begin{align}
    \min_{\alpha \geq 0} \varphi(\alpha) = f(x^{(k)} + \alpha d^{(k)})
\end{align}
\subsubsection{Exact Line Search}
\par
Solve Line Search problem in finite iterations.
\par
\subsubsection{Inexact Line Search}
\par
In some cases, the exact solution of Line Search
is not necessary, so we can use inexace line search
to improve algorithm efficiency.
\par
\noindent\textit{Goldstein Conditions}
\begin{align}
    \varphi(\alpha) &\leq \varphi(0) + \rho\alpha\varphi'(0) \\
    \varphi(\alpha) &\geq \varphi(0) + (1 - \rho)\alpha\varphi'(0)
\end{align}
where$\rho \in (\frac{1}{2}, 1)$ is a fixed parameter.
\par
However, the downside of Goldstein Conditions
is that the optimal value might not lie in
the valid area.
\par\noindent\textit{Wolfe-Powell Conditions}
\begin{align}
    \varphi(\alpha) &\leq \varphi(0) + \rho\alpha\varphi'(0) \\
    \varphi'(\alpha) &\geq \sigma\varphi'(0)
\end{align}
where $\sigma \in (\rho, 1)$.

\subsection{Global Convergence}
\begin{theorem}
    Assume $\bigtriangledown f(x)$ exists and uniformly continuous
    on level set $L(x^{(0)}) = \{ x | f(x) \leq f(x^{(0)})\}$.
    Denote $\theta^{(k)}$ as the angle between $d^{(k)}$
    and $-\bigtriangledown f(x^{(k)})$.
    \begin{align}
        \theta^{(k)} \leq \frac{\pi}{2} - \mu
    \end{align}
    If step factor is determined by following methods
    \begin{itemize}
        \item Exace Line Search
        \item Goldstein Conditions
        \item Wolfe-Powell Conditions
    \end{itemize}
    Then, there exists $k$, such that $\bigtriangledown f(x^{(k)}) = 0$,
    or $f(x^{(k)}) \rightarrow 0$ or $f(x^{(k)}) \rightarrow - \infty$.
\end{theorem}
\begin{proof}
    
\end{proof}

\subsection{Steepest Descent Method}
Steepest Descent Method is a Line Search Method.
\begin{align}
    x^{(k+1)} = x^{(k)} - \alpha_k \bigtriangledown f(x^{(k)})
\end{align}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Termination error $\epsilon$, cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, k:= 0$\;
     \While{$\parallel g^{(k)} \parallel \geq \epsilon$}{
        $d^{(k)} = - g^{(k)}$\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$, $k:= k+1$\;
        Compute $g^{(k)} = \bigtriangledown f(x^{(k)})$
     }
     \caption{Steepest Descent Algorithm}
\end{algorithm}
Steepest Descent Method has linear convergence rate generally.

\subsection{Newton Method}

Newton Method is also a Line Search Method.
\begin{align}
    f(x^{(k)} + s) \approx q^{(k)}(s) f(x^{(k)}) + g^{(k)^T} s
     + \frac{1}{2} s^T G_k s
\end{align}
where $g^{(k)} = \bigtriangledown f(x^{(k)})$,
$G_k = \bigtriangledown^2 f(x^{(k)})$.
To minimize $q^{(k)}(s)$, we have
\begin{align}
    s = G_k^{-1} g^{(k)}
\end{align}
Notice that $G_k^{-1} g^{(k)}$ is the Newton Direction.
\par
\vspace{.3em}
\noindent\textit{Analysis on quadratic function}
\par
For positive definite quadratic function
\begin{align}
    f(x) = \frac{1}{2} x^TGx - c^Tx
\end{align}
In this case, $\bigtriangledown^2 f(x) = G$.
Let $H_0 = G^{-1}$, then we have
\begin{align}
    d^{(0)} &= H_0 \bigtriangledown f(x^{(0)}) \\
    &= G^{-1} (Gx^{(0)} - c) \\
    &= x^{(0)} - G^{-1}c \\
    &= x^{(0)} - x^*
\end{align}
So that Newton Method can reach global optimal
in $1$ iteration for quadratic functions.
\par
For general non-linear functions, if we follow
\begin{align}
    x^{(k+1)} = x^{(k)} - G_k^{-1} g^{(k)}
\end{align}
we called it Newton Method.
\par
\vspace{.3em}
\noindent\textit{Convergence Rate of Newton Method}
\begin{theorem}
    $f \in \mathcal{C}^2$, $x^{(k)}$ is sufficiently closed
    to optimal point $x^*$, where $\bigtriangledown f(x^*) = 0$.
    If $\bigtriangledown^2 f(x^*)$ is positive definite, 
    Hesse matrix of $f$ satisfies Lipschitz Condition, i.e.,
    $\exists \beta >0$, such that for all $(i, j)$,
    \begin{align}
        | G_{ij}(x) - G_{ij}(y)| \leq \beta \parallel x - y \parallel
    \end{align}
    Then $\{x^{(k)}\} \rightarrow x^*$, and have quadratic
    convergence rate.
\end{theorem}
\begin{proof}
    Denote $g(x) = \bigtriangledown f(x)$, then we have
    \begin{align}
        g(x - h) = g(x) - G(x)h + O(\parallel h \parallel^2)
    \end{align}
    Let $x = x^{(k)}$, $h = h^{(k)} = x^{(k)} - x^*$, then
    \begin{align}
        g(x^*) = g(x^{(k)}) - G(x^{(k)})(h^{(k)})
        + O(\parallel h^{(k)} \parallel^2) = 0
        \label{equ:newton1}
    \end{align}
    From Lipschitz Condition, we can easily get 
    $G(x^{(k)})^{-1}$ is finite. Then we left multiply
    $G(x^{(k)})^{-1}$ to Equation (\ref{equ:newton1})
    \begin{align}
        0 &= G(x^{(k)})^{-1} g(x^{(k)}) - h^{(k)}
         + O(\parallel h^{(k)} \parallel^2) \\
        &= x^* - x^{(k)} + G(x^{(k)})^{-1} g(x^{(k)})
         + O(\parallel h^{(k)} \parallel^2) \\
         &= x^* - x^{(k+1)}
         + O(\parallel h^{(k)} \parallel^2) \\
         &= - h^{(k+1)} + O(\parallel h^{(k)} \parallel^2)
    \end{align}
    i.e.,
    \begin{align}
        \parallel h^{(k+1)} \parallel = O(\parallel h^{(k)} \parallel^2)
    \end{align}
\end{proof}

\subsection{Quasi-Newton Method}
Newton Method has a fast convergence rate.
However, Newton Method requires second-order derivative,
if Hesse matrix is not positive definite, Newton Method
might not work well.
\par
In order to overcome the above difficulties,
Quasi-Newton Method is introduced.
Its basic idea is that:
Using second-order derivative free matrix $H_k$
to approximate $G(x^{(k)})^{-1}$.
Denote $s^{(k)} = x^{(k+1)} - x^{(k)}$,
$y^{(k)} = \bigtriangledown f(x^{(k+1)}) - \bigtriangledown f(x^{(k)})$,
then we have
\begin{align}
    \bigtriangledown^2 f(x^{(k)}) s^{(k)} \approx y^{(k)}
\end{align}
or
\begin{align}
    \bigtriangledown^2 f(x^{(k)})^{-1} y^{(k)} \approx s^{(k)}
\end{align}
So we need to construct $H_{k+1}$ such that
\begin{align}
    H_{k+1} y^{(k)} \approx s^{(k)}
    \label{equ:quasi-newton-condition1}
\end{align}
or
\begin{align}
    y^{(k)} \approx B_{k+1} s^{(k)}
    \label{equ:quasi-newton-condition2}
\end{align}
we called (\ref{equ:quasi-newton-condition1}), (\ref{equ:quasi-newton-condition2})
\textit{Quasi-Newton Conditions}
or \textit{Secant Conditions}.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, H_0 = I, k:= 0$\;
     \While{some conditions}{
        $d^{(k)} = - H_kg^{(k)}$\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$\;
        generate $H_{k+1}$, $k:= k+1$
     }
     \caption{Quasi-Newton Algorithm}
\end{algorithm}
\vspace{.3em}
\subsubsection{How to generate $H_k$}
\par
$H_k$ is the approximation matrix in $k$th iteration, we want to
generate $H_{k+1}$ from $H_k$
\paragraph{Symmetric Rank 1 Update}
Assume
\begin{align}
    H_{k+1} = H_k + a \mathbf{u}\mathbf{u}^T,
     \quad a \in \mathbb{R}, \mathbf{u} \in \mathbb{R}^n
\end{align}
From the Quasi-Newton Conditions, we have
\begin{align}
    H_{k+1} \mathbf{y}^{(k)} &= \mathbf{s}^{(k)} \\
    H_k \mathbf{y}^{(k)} + a \mathbf{u}\mathbf{u}^T \mathbf{y}^{(k)}
    &= \mathbf{s}^{(k)} \\
    H_k \mathbf{y}^{(k)} + a \mathbf{u}^T\mathbf{y}^{(k)} \mathbf{u} 
    &= \mathbf{s}^{(k)}
\end{align}
Let $\mathbf{u} = \mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)}$, 
$a = \frac{1}{\mathbf{u}^T \mathbf{y}}$,
clearly this is a solution of the equation.
Here we have
\begin{align}
    H_{k+1} = \frac{(\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})
    (\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})^T}
    {(\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})^T \mathbf{y}^{(k)}}
    \label{equ:quasi-newton3}
\end{align}
(\ref{equ:quasi-newton3}) is \textit{Symmetric Rank 1 Update}.
The problem of Symmetric Rank 1 Update is that
the positive-definite property of $H_k$
can not be preserved.

\paragraph{Symmetric Rank 2 Update}
Assume
\begin{align}
    H_{k+1} = H_k + a \mathbf{u}\mathbf{u}^T +
     b \mathbf{v}\mathbf{v}^T,
     \quad a, b \in \mathbb{R}, \mathbf{u}, \mathbf{v} \in \mathbb{R}^n
\end{align}
such that Quasi-Newton Conditions stand.
We can find a solution of $a, b, \mathbf{u}, \mathbf{v}$ that is
\begin{align}
    \left\{
    \begin{array}{ll}
        \mathbf{u} = \mathbf{s}^{(k)}, 
        \quad a \mathbf{u}^T \mathbf{y} = 1 \\
        \mathbf{v} = H_k \mathbf{y}^{(k)}, 
        \quad b \mathbf{v}^T \mathbf{y} = -1
    \end{array}
    \right.
\end{align}
So that we have
\begin{align}
    H_{k+1} = H_k + \frac{\mathbf{s}^{(k)}\mathbf{s}^{(k)T}}
    {\mathbf{s}^{(k)T} \mathbf{y}^{(k)}} - 
    \frac{H_k\mathbf{y}^{(k)} \mathbf{y}^{(k)T} H_k}
    {\mathbf{y}^{(k)T}H_k\mathbf{y}^{(k)}}
    \label{equ:quasi-newton-dfp}
\end{align}
We called (\ref{equ:quasi-newton-dfp}) the DFP (Davidon-Fletcher-Powell) update.
\par
From Quasi-Newton Condition (\ref{equ:quasi-newton-condition2}), we can get the
BFGS (Broyden-Fletcher-Goldfarb-Shanno) update
\begin{align}
    B_{k+1}^{(BFGS)} = B_k +
    \frac{\mathbf{y}^{(k)}\mathbf{y}^{(k)T}}
    {\mathbf{y}^{(k)T} \mathbf{s}^{(k)}}
    - \frac{B_k \mathbf{s}^{(k)}\mathbf{s}^{(k)T} B_k}
    {\mathbf{s}^{(k)T} B_k \mathbf{s}^{(k)}}
\end{align}
\vspace{.3em}
\par\noindent\textit{Inverse of SR1 update}
\begin{theorem}[Sherman-Morrison]
    $A \in \mathbb{R}^n \times \mathbb{R}^n$ is a 
    non-singular matrix, $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$.
    If $1 + \mathbf{v}^TA^{-1}\mathbf{u} \neq 0$,
    then SR1 update of $A$ is non-singular, and its inverse
    can be represented as
    \begin{align}
        (A + a\mathbf{u}\mathbf{v}^T)^{-1} = 
        A^{-1} - \frac{A^{-1}\mathbf{u}\mathbf{v}^TA^{-1}}
        {1 + \mathbf{v}^TA^{-1}\mathbf{u}}
    \end{align}
\end{theorem}

\subsection{Conjugate Gradient Method}
\begin{definition}{Conjugate Direction.}
    $G$ is a $n \times n$ positive definite matrix, 
    \par\noindent for non-zero vector set
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\} \in \mathbb{R}^n$,
    if $\mathbf{d}^{(i)T} G \mathbf{d}^{(j)} = 0, (i \neq j)$,
    then we called $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$
    is \emph{G-Conjugate}.
    \label{def:Conjugate}
\end{definition}

\begin{lemma}
    For non-zero conjugate vector set 
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\} \in \mathbb{R}^n$,
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ are linearly independent.
\end{lemma}
\begin{proof}
    From Definition \ref{def:Conjugate}, we have
    \begin{align}
        \mathbf{d}^{(i)T} G \mathbf{d}^{(j)} = 0, \forall i, j, i \neq j
    \end{align}
    if $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ is linearly dependent,
    there exists 
    \begin{align}
        \mathbf{d}^{(t)} = \sum_{j=0}^k c_j \mathbf{d}^{(j)}
    \end{align} 
    then
    \begin{align}
        \mathbf{d}^{(t)T} G \mathbf{d}^{(i)} = 
        \sum_{j=0}^k c_j \mathbf{d}^{(j)} G \mathbf{d}^{(i)}
        = c_i \mathbf{d}^{(i)} G \mathbf{d}^{(i)} \neq 0
    \end{align}
    so that $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$
    are linearly independent.
    
    \label{lemma:Conjugate}
\end{proof}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n$, positive definite matrix $G$, $k:= 0$\;
     Construct $\mathbf{d}^{(0)}$ such that
     $\mathbf{g}^{(0)T}\mathbf{d}^{(0)} < 0$\;
     \While{some conditions}{
        solve $\min_{\alpha_k \geq 0} f(\mathbf{x}^{(k)} + \alpha_k \mathbf{d}^{(k)})$\;
        $\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k\mathbf{d}^{(k)}$\;
        Construct $\mathbf{d}^{(k+1)}$ such that 
        $\mathbf{d}^{(k+1)}G\mathbf{d}^{(j)} = 0, j = 0,..., k$.\;
        $k:=k+1$
     }
     \caption{Conjuagte Gradient Algorithm}
\end{algorithm}

\begin{theorem}[Conjugate Gradient]
    For strictly convex quadratic function 
    \par\noindent
    $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T G \mathbf{x} + \mathbf{c}^T\mathbf{x}$,
    apply conjugate gradient method combined with exact line search,
    then $\mathbf{x}^{(k+1)}$ is the global minima in manifold
    \begin{align}
        \mathcal{V} =  \{ \mathbf{x} | \mathbf{x} = \mathbf{x}^{(0)} + 
        \sum_{j=0}^k\beta_j\mathbf{d}^{(j)}, \forall \beta_j \in \mathbb{R} \}
    \end{align}
    \label{th:Conjugate}
\end{theorem}
\begin{proof}
    Firstly, from Lemma \ref{lemma:Conjugate}, we have 
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ are linearly independent.
    So we only need to prove that 
    for all $k < n$
    \begin{align}
        \mathbf{g}^{(k+1)T}\mathbf{d}^{(j)} = 0, j = 0,..., k
    \end{align}
    i.e., $\mathbf{g}^{(k+1)}$ is orthogonal with subspace
    $span\{\mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$.
    \par
    Due to the exact line search, $\forall j$
    \begin{align}
        \mathbf{g}^{(j+1)T}\mathbf{d}^{(j)} = 0
    \end{align}
    especially $\mathbf{g}^{(k+1)T}\mathbf{d}^{(k)} = 0$.
    \par
    Notice that 
    \begin{align}
        \mathbf{g}^{(k+1)} - \mathbf{g}^{(k)}
        = G (\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)})
        = \alpha_k G \mathbf{d}^{(k)}
    \end{align}
    so that we have $\forall j \leq k$
    \begin{align}
        \mathbf{g}^{(k+1)T}\mathbf{d}^{(j)}
        &= (\sum_{m=j+1}^k (\mathbf{g}^{(m+1)T} - \mathbf{g}^{(m)T})
        + \mathbf{g}^{(j+1)T} )\mathbf{d}^{(j)} \\
        &= \sum_{m=j+1} \alpha_m \mathbf{d}^{(m)T}G \mathbf{d}^{(j)}
        + \mathbf{g}^{(j+1)T} \mathbf{d}^{(j)} \\
        &= 0
    \end{align}
    
\end{proof}
\begin{lemma}
    For strictly convex quadratic function 
    $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T G \mathbf{x} + \mathbf{c}^T\mathbf{x}$,
    apply conjugate gradient method combined with exact line search,
    $\mathbf{g}(\mathbf{x}) = \bigtriangledown f(\mathbf{x}) = G\mathbf{x} + \mathbf{c}$,
    we have
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)} = 0, \forall j = 0,..., k-1
    \end{align}
    \label{lemma:Conjugate}
\end{lemma}
\begin{proof}
    From Theorem \ref{th:Conjugate}, we have
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)}
        = \mathbf{g}^{(k)T} (-\mathbf{d}^{(j)} 
        + \sum_{i=0}^{j-1}\beta_i^{(j)} \mathbf{d}^{(i)}) = 0
    \end{align}
    
\end{proof}
\subsubsection{Quadratic function case}
\par
For $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T G \mathbf{x} + \mathbf{c}^T x$,
$G$ is a $n \times n$ positive definite matrix.
\begin{align}
    \mathbf{g}(\mathbf{x}) = G \mathbf{x} + \mathbf{c}
\end{align}
Set $\mathbf{d}^{(0)} = -\mathbf{g}^{(0)}$,
exact line search for $\alpha_0$ such that 
$\mathbf{g}^{(1)T}\mathbf{d}^{(0)} = 0$.
Assume $\mathbf{d}^{(1)} = -\mathbf{g}^{(1)} + \beta_0^{(1)}\mathbf{d}^{(0)}$,
select $\beta_0^{(1)}$ such that
$ \mathbf{d}^{(1)} G \mathbf{d}^{(0)} = 0$
\begin{align}
    \beta_0^{(1)} = \frac{\mathbf{g}^{(1)T}\mathbf{g}^{(1)}}
    {\mathbf{g}^{(0)T}\mathbf{g}^{(0)}}
\end{align}
\begin{proof}
    From (92), we have
    \begin{align}
        &\mathbf{d}^{(1)T} G \mathbf{d}^{(0)} = 0 \\
        \Leftrightarrow \quad&\mathbf{d}^{(1)T}
        (\mathbf{g}^{(1)} - \mathbf{g}^{(0)}) = 0 \\
        \Leftrightarrow  \quad&(\mathbf{g}^{(1)} + 
        \beta_0^{(1)}\mathbf{g}^{(0)})^T
        (\mathbf{g}^{(1)} - \mathbf{g}^{(0)}) = 0 \\
        \Leftrightarrow \quad&\mathbf{g}^{(1)T}\mathbf{g}^{(1)}
        - \beta_0^{(1)}\mathbf{g}^{(0)T}\mathbf{g}^{(0)} = 0 \\
        \Leftrightarrow \quad&\beta_0^{(1)} = 
        \frac{\mathbf{g}^{(1)T}\mathbf{g}^{(1)}}
        {\mathbf{g}^{(0)T}\mathbf{g}^{(0)}}
    \end{align}
    
\end{proof}
    Generally, we can select $\beta_j^{(k)}$ such that
    $\mathbf{d}^{(k)T} G \mathbf{d}^{(j)} = 0, j=0, 1,..., k-1$
    that is
    \begin{align}
        \mathbf{d}^{(k)T} G \mathbf{d}^{(j)} &= 0 \\
        (-\mathbf{g}^{(k)T} + \sum_{i=0}^{k-1} \beta_i^{(k)} \mathbf{d}^{(i)T})
        G \mathbf{d}^{(j)} &= 0 \\
        -\mathbf{g}^{(k)T} G \mathbf{d}^{(j)}
        + \beta_j^{(k)}\mathbf{d}^{(j)T} G \mathbf{d}^{(j)} &= 0
    \end{align}
    so we have
    \begin{align}
        \beta_j^{(k)} = \frac{\mathbf{g}^{(k)T} G \mathbf{d}^{(j)}}
        {\mathbf{d}^{(j)T} G \mathbf{d}^{(j)}}
        = \frac{\mathbf{g}^{(k)T} (\mathbf{g}^{(j+1)} - \mathbf{g}^{(j)})}
        {\mathbf{d}^{(j)T}(\mathbf{g}^{(j+1)} - \mathbf{g}^{(j)})}
    \end{align}
    From Lemma \ref{lemma:Conjugate}, we have 
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)} = 0, \forall j = 0,..., k-1
    \end{align}
    So
    \begin{align}
        \beta_j^{(k)} &= 0, j = 0,...,k-2 \\
        \beta_{k-1}^{(k)} &= \frac{\mathbf{g}^{(k)T} (\mathbf{g}^{(k)}}{\mathbf{g}^{(k-1)T} (\mathbf{g}^{(k-1)}}
    \end{align}

\section{Trust Region Method}
Previously, we use a direction search strategy
to determine a search direction, then use line
search method to determine step length.
\par
Now we discuss a new global convergence strategy --
Trust-Region Method.
\begin{definition}[Trust Region]
    \begin{align}
        \Omega_k = \{ \mathbf{x} \in \mathbb{R}^n \ | \ \parallel
        \mathbf{x} - \mathbf{x}^{(k)} \parallel \leq e_k  \}
    \end{align}
    We called $\Omega_k$ \emph{Trust Region}, $e_k$ is the \emph{Trust radius}.
\end{definition}

Suppose in this neighborhood, quadratic model $q^{(k)}(\mathbf{s})$
is a proper approximation of $f(\mathbf{x})$.
We minimize the quadratic model in trust region, derive
approximate minima $\mathbf{s}^{(k)}$,
and set $\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \mathbf{s}^{(k)}$.
\par
\subsection{Trust Region Subproblem}
\begin{align}
    \min_{\parallel \mathbf{s} \parallel \leq e_k} q^{(k)}(\mathbf{s})
     = f(\mathbf{x}^{(k)}) + \mathbf{g}^{(k)T}\mathbf{s}
    + \frac{1}{2} \mathbf{s}^T B_k \mathbf{s}
\end{align}

Where $\mathbf{s} = \mathbf{x} - \mathbf{x}^{(k)}$,
$\mathbf{g}^{(k)} = \bigtriangledown f(\mathbf{x}^{(k)})$,
$B_k = \bigtriangledown^2 f(\mathbf{x}^{(k)})$.
$e_k$ is the trust region radius.

\par
\subsection{How to select $e_k$}
\par
Denote the solution of the subproblem as $\mathbf{s}^{(k)}$,
then let
\begin{align}
    \textnormal{Act}_k = f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k)} + \mathbf{s}^{(k)}) \\
    \textnormal{Pre}_k = q^{(k)}(\mathbf{0}) - q^{(k)}(\mathbf{s}^{(k)}) 
\end{align}

Define
\begin{align}
    r_k = \frac{\textnormal{Act}_k}{\textnormal{Pre}_k}
    = \frac{f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k)} + \mathbf{s}^{(k)})}
    {q^{(k)}(\mathbf{0}) - q^{(k)}(\mathbf{s}^{(k)})}
\end{align}

to measure the difference between objective function
and the quadratic approximate model.
\par
We can update $e_k$ according to $r_k$. If $r_k$ is too small,
that means our model can not fit the objective function well,
so we need to decrease $e_k$. If $r_k$ is close to $1$,
that means out model is good and we can increase $r_k$.
Set the parameters $0 < \gamma_1 < \gamma_2 < 1$ and 
$0 < \eta_1 < 1 < \eta_2$, we can have the following update
rule
\begin{align}
    e_{k+1} = \left\{
        \begin{array}{lll}
            &\eta_1 e_k &\textnormal{if} \ r_k < \gamma_1 \\
            &e_k &\textnormal{if} \ \gamma_1 < r_k < \gamma_2 \\
            &\min(\eta_2 e_k, \bar{e}) \quad &\textnormal{if} \ r_k \geq \gamma_2
        \end{array}\right.
\end{align}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n$, $e_0 \in (0, \bar{e})$, $\epsilon > 0$,
     $0 < \gamma_1 < \gamma_2 < 1$, $0 < \eta_1 < 1 < \eta_2$, $k:= 0$\;
     \While{$\parallel \mathbf{g}^{(k)} \parallel \geq \epsilon$}{
        solve the subproblem to derive $\mathbf{s}^{(k)}$\;
        calculate $r_k$, update $\mathbf{x}$\;
        \eIf{$r_k > 0$}
        {$\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \mathbf{s}^{(k)}$}
        {$\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)}$}
        update $e_k$ following (117)\;
        $k:=k+1$\;
     }
     \caption{Trust Region Algorithm}
\end{algorithm}

% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage{multirow, multicol}
\usepackage[ruled,vlined]{algorithm2e}
    
% \newtheorem{theorem}{Theorem}
% \newtheorem{theorem}{Theorem}[section]

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV18SubNumber{1452}  % Insert your submission number here

\title{Optimization Algorithm Notes} % Replace with your title

\titlerunning{ECCV-18 submission ID \ECCV18SubNumber}

\authorrunning{ECCV-18 submission ID \ECCV18SubNumber}

\author{Depu Meng}
\institute{Feb. $2019$}


\maketitle
\section{Introduction to Optimization Algorithms}
\subsection{Goal of the Course}
\begin{itemize}
    \item Understand foundations of optimization
    \item Learn to analyze widely used optimization algorithms
    \item Be familiar with implementation of optimization algorithms
\end{itemize}

\subsection{Topics Involved}
\begin{itemize}
    \item Unconstrained optimization
    \item Constrained optimization
    \item Convex optimization
    \item Sparse optimization
    \item Stochastic optimization
    \item Combinational optimization
    \item Global optimization
\end{itemize}

\subsection{Basic Concepts}
\noindent\textbf{Problem Definition}
Find the value of the decision variable s.t.
objective function is maximized/minimized
under certain conditions.
\begin{align}
    & \min f(x) \\
    & s.t. x \in \mathcal{S} \subset \mathbb{R}^n
\end{align}
Here, we call $\mathcal{S}$ \textit{feasible region}.
\par
We often denote constrained optimization Problem
 as 
\begin{align}
    & \min f(x) \\
    s.t.\quad & g_i(x) \geq 0, i = 1,..., n \\
    & b_i(x) = 0, i \in 1, ..., m
\end{align}

\begin{definition}{Global Optimality.}
    For global optimal value $x^* \in \mathcal{S}$,
    \begin{align}
        f(x^*) \leq f(x), \forall x \in \mathcal{S}
    \end{align}
\end{definition}

\begin{definition}{Local Optimality.}
    For local optimal value $x^* \in \mathcal{S}$, $\exists U(x^*)$,
    such that
    \begin{align}
        f(x^*) \leq f(x), \forall x \in \mathcal{S} \cap U(x^*)
    \end{align}
    \end{definition}

\begin{definition}{Feasible direction.}
    Let $x \in \mathcal{S}$, $d \in \mathbb{R}^n$
    is a non-zero vector. if $\exists \delta > 0$,
    such that
    \begin{align}
        x + \lambda d \in \mathcal{S}, \forall \lambda \in (0, \delta)
    \end{align}
    Then $d$ is a \textbf{feasible direction} at $x$.
    We denote $F(x, \mathcal{S})$ as the set of feasible directions
    at $x$.
\end{definition}

\begin{definition}{Descent direction.}
    $f(x): \mathbb{R}^n \rightarrow \mathbb{R}$, $x \in \mathbb{R}^n$,
    $d$ is a non-zero vector. If $\exists \delta > 0$, such that
    \begin{align}
        f(x + \lambda d) < f(x), \forall \lambda \in (0, \delta)
    \end{align}
    Then $d$ is a \textbf{descent direction} at $x$.
    We denote $D(x, f) = \{ d | \bigtriangledown f(x)^T d < 0 \}$
    as the set of descent direction at $x$.
\end{definition}

\subsection{Optimal Conditions}
\par\noindent
\textbf{Unconstrained Optimization}
\par
First-order necessary condition:
$f(x)$ is differentiable at $x$, 
\begin{align}
    \bigtriangledown f(x) = 0
\end{align}
\par
Second-order necessary condition:
$f(x)$ is second-order differentiable at $x$,
\begin{align}
    \bigtriangledown f(x) = 0 \\
    \bigtriangledown^2 f(x) \geq 0
\end{align}
\noindent
\textbf{Constrained Optimization}


\begin{theorem}{Fritz-John Condition}
    \par
    For constrained optimization problem
    \begin{align}
        & \min f(x) \\
        s.t.\quad & g_i(x) \geq 0, i = 1,..., n \\
        & h_i(x) = 0, i \in 1, ..., m
    \end{align}
    Denote $I(x) = \{i \in \{1,...,n\} | g_i(x) = 0\}$.
    For $x \in \mathcal{S}$, $f$ and $g_i, i \in I(x)$
    is differentiable at $x$, $h_j(x)$ is continuously
    differentiable at $x$.
    If $x$ is local optimal, then there exists non-trivial
    $\lambda_0, \lambda_i \geq 0, i \in I(x)$
    and $\mu_j$, such that
    \begin{align}
        \lambda_0 \bigtriangledown f(x) - \sum_{i \in I(x)} \lambda_i
        \bigtriangledown g_i(x) - \sum_{j=1}^m \mu_j
        \bigtriangledown h_j(x) = 0
    \end{align}
\end{theorem}

\begin{proof}
    (i) If $\{\bigtriangledown h_j (x)\}$ is linearly dependent,
    then there exists non-trivial $\mu_j$, such that
    \begin{align}
        \sum_{j=1}^m \bigtriangledown \mu_jh_j (x) = 0
    \end{align}
    Let $\lambda_0, \lambda_i, i \in I(x) = 0$, then (13) holds.
    \par
    (ii) If $\{\bigtriangledown h_j (x)\}$ is linearly independent,
    Denote
    \begin{align}
        F_g &= F(x, g) = \{ d | \bigtriangledown g_i(x)^Td > 0, i \in I(x)\} \\
        F_h &= F(x, h) = \{ d | \bigtriangledown h_j(x)^Td = 0, j = 1,...,m\}
    \end{align}
    If $x$ is a optimal value, then appearently 
    $F(x, \mathcal{S}) \cap D(x, f) = \varnothing$.
    Due to the independence of $\{\bigtriangledown h_j (x)\}$,
    we have {\color{red}$F_g \cap F_h \subset F(x, \mathcal{S})$},
    then 
    \begin{align}
        F_g \cap F_h \cap D(x, f) = \varnothing
    \end{align}
    that is
    \begin{align}
        \left\{
        \begin{array}{ll}
            \bigtriangledown f(x)^Td < 0 \\
            \bigtriangledown g_i(x)^Td > 0, i \in I(x) \\
            \bigtriangledown h_j (x)^Td  =0, j = 1,...,m
        \end{array}
        \right.
    \end{align}
    has no solution.
    Let 
    \begin{align}
        A &= \{ \bigtriangledown f(x)^T, -\bigtriangledown g_i(x)\}^T, i \in I(x) \\
        B &= \{ -\bigtriangledown h_j(x)\}, j = 1,...,m
    \end{align}
    Then (21) is equivalent to
    \begin{align}
        \left\{
        \begin{array}{ll}
            A^T d < 0 \\
            B^T d = 0
        \end{array}
        \right.
    \end{align}
    has no solution.
    \par
    \noindent
    Denote
    \begin{align}
        S_1 &= 
        \left\{
            \left(
            \begin{array}{ll}
                y_1 \\
                y_2
            \end{array}
            \right)
            | y_1 = A^Td, y_2 = B^Td, d \in \mathbb{R}^n
        \right\} \\
        S_2 &= 
        \left\{
            \left(
            \begin{array}{ll}
                y_1 \\
                y_2
            \end{array}
            \right)
            | y_1 < 0, y_2 = 0
        \right\} 
    \end{align}
    $S_1, S_2$ are non-trivial convex sets,
    and $S_1 \cap S_2 = \varnothing$.
    From \textit{Hyperplane Separation Theorem}:
    $\exists 
    \left(
    \begin{array}{ll}
        p_1 \\
        p_2
    \end{array}
    \right)$, such that
    \begin{align}
        p_1^TA^Td + p_2^TB^Td \geq p_1^Ty_1 + p_2^Ty_2,
        \forall d \in \mathbb{R}^n, 
        \forall\left(
        \begin{array}{ll}
            y_1 \\
            y_2
        \end{array}
        \right) \in CL(S_2)
    \end{align}
    Let $y_2 = 0, d = 0, y_1 < 0$, we have
    \begin{align}
        p_1 \geq 0
    \end{align}
    Let $\left(
        \begin{array}{ll}
            y_1 \\
            y_2
        \end{array}
        \right) = 
        \left(
        \begin{array}{ll}
            0 \\
            0
        \end{array}
        \right) 
         \in CL(S_2)$
    So that
    \begin{align}
        (p_1^TA^T + p_2^TB^T)d \geq 0 \\
        (Ap_1 + Bp_2)^Td \geq 0
    \end{align}
    Let $d = - (Ap_1 + Bp_2)$, we have
    \begin{align}
        Ap_1 + Bp_2 = 0
    \end{align}
    From above, we have
    \begin{align}
        \left\{
        \begin{array}{ll}
            Ap_1 + Bp_2 = 0 \\
            p_1 \geq 0
        \end{array}
        \right.
    \end{align}
    Let $p_1 = \{\lambda_0,..., \lambda_{I(x}\}$,
    $p_2 = \{\mu_1, ..., \mu_m\}$, i.e.,
    \begin{align}
        \left\{
            \begin{array}{ll}
            \lambda_0 \bigtriangledown f(x) - \sum_{i \in I(x)} \lambda_i
            \bigtriangledown g_i(x) - \sum_{j=1}^m \mu_j
            \bigtriangledown h_j(x) = 0 \\
            \lambda_i \geq 0
            \end{array}
            \right.
    \end{align}
\end{proof}

\begin{theorem}{Kuhn-Tucker Condition}
    \par
    For constrained optimization problem
    \begin{align}
        & \min f(x) \\
        s.t.\quad & g_i(x) \geq 0, i = 1,..., n \\
        & h_i(x) = 0, i \in 1, ..., m
    \end{align}
    Denote $I(x) = \{i \in \{1,...,n\} | g_i(x) = 0\}$.
    For $x \in \mathcal{S}$, $f$ and $g_i, i \in I(x)$
    is differentiable at $x$, $h_j(x)$ is continuously
    differentiable at $x$.
    $\{\bigtriangledown g_i(x), i \in I(x);\bigtriangledown h_j(x), 
    j = 1,...,m\}$ is linearly independent.
    If $x$ is local optimal, then $\exists \lambda_i \geq 0$
    and $\mu_j$, such that
    \begin{align}
        \bigtriangledown f(x) - \sum_{i \in I(x)} \lambda_i
        \bigtriangledown g_i(x) - \sum_{j=1}^m \mu_j
        \bigtriangledown h_j(x) = 0
    \end{align}
\end{theorem}
\subsection{Descent function}
\par
\begin{definition}{Descent function.}
    Denote solution set $\Omega \in X$, $\mathcal{A}$ is an
    algorithm on $X$, $\psi: X \rightarrow \mathbb{R}$.
    If
    \begin{align}
        \psi(y) < \psi(x),\quad \forall x \notin \Omega, y \in \mathcal{A}(x) \\
        \psi(y) \leq \psi(x),\quad \forall x \in \Omega, y \in \mathcal{A}(x)
    \end{align}
    Then $\psi$ is a \textbf{descent function} of $(\Omega, \mathcal{A})$.
\end{definition}
\subsection{Convergence of Algorithm}
\par
\begin{theorem}
    $\mathcal{A}$ is an algorithm on $X$,
    $\Omega$ is the solution set, $x^{(0)} \in X$.
    If $x^{(k)} \in \Omega$, then the iteration stops.
    Otherwise set $x^{(k+1)} = \mathcal{A}(x^{(k)}), k:=k+1$.
    If
    \begin{itemize}
        \item $\{x^{(k)}\}$ in a compact subset of $X$
        \item There exists a continuous function $\psi$,
         $\psi$ is a descent function of $(\Omega, \mathcal{A})$
        \item $\mathcal{A}$ is closed on $\Omega^C$
    \end{itemize}
    Then, any convergent subsequence of $\{x^{(k)}\}$ converges to 
    $x, x \in \Omega$.
\end{theorem}
\begin{proof}
    
\end{proof}

\subsection{Search Methods}
\noindent\textbf{Line Search}
\par
Generate $d^{(k)}$ from 
$x^{(k)}$, 
\begin{align}
    x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}
\end{align}.
search $\alpha_k$ in 1-D space.
\par
\noindent\textbf{Trust Region}
\par
Generate local model $Q_{k}(s)$ of $x^{(k)}$,
\begin{align}
    s^{(k)} = \mathop{\arg\min} Q_k(s) \\
    x^{(k+1)} = x^{(k)} + s^{(k)}
\end{align}

\clearpage
\section{Unconstrained Optimization}

\subsection{Gradient Based Methods}

\begin{align}
    \min_{x \in \mathbb{R}^n} f(x)
\end{align}
\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Solution set $\Omega$, cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, k:= 0$\;
     \While{$x^{(k)} \notin \Omega$}{
      $d^{(k)} = - H_k \bigtriangledown f(x^{(k)})$,
        ($H_k$ is a positive definite symmetrical matrix)\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$, $k:= k+1$
     }
     \caption{Example of gradient based algorithm}
\end{algorithm}
\subsection{Determine Search Direction}
\noindent\textbf{First-order gradient method}
\par
For unconstrained optimization problem 
\begin{align}
    \min_{x \in \mathbb{R}^n} f(x)
\end{align}
We have
\begin{align}
    f(x) = f(x^{(k)}) + \bigtriangledown f(x^{(k)})^T(x - x^{(k)})
    + O(\parallel x - x^{(k)} \parallel^2)
\end{align}
Set $d^{(k)} = - \bigtriangledown f(x^{(k)})$,
when $\alpha_k$ is sufficiently small,
\begin{align}
    f(x^{(k)} + \alpha_k d^{(k)}) < f(x^{(k)})
\end{align}

\noindent\textbf{Second-order gradient method -- Newton Direction}
\par
\begin{align}
    f(x) = &f(x^{(k)}) + \bigtriangledown f(x^{(k)})^T(x - x^{(k)}) \\
    &+ \frac{1}{2} (x - x^{(k)})^T \bigtriangledown^2 f(x^{(k)}) (x - x^{(k)})
    + O(\parallel x - x^{(k)} \parallel^3)
\end{align}
Set $d^{(k)} = -G_k^{-1} \bigtriangledown f(x^{(k)})$,
 where $G_k = \bigtriangledown^2 f(x^{(k)})$,
 i.e., Hesse matrix of $f$ at $x^{(k)}$.
\subsection{Determine Step Factor -- Line Search}

\begin{align}
    \min_{\alpha \geq 0} \varphi(\alpha) = f(x^{(k)} + \alpha d^{(k)})
\end{align}
\noindent\textbf{Exact Line Search}
\par
Solve Line Search problem in finite iterations.
\par
\noindent\textbf{Inexact Line Search}
\par
In some cases, the exact solution of Line Search
is not necessary, so we can use inexace line search
to improve algorithm efficiency.
\par
\noindent\textit{Goldstein Conditions}
\begin{align}
    \varphi(\alpha) &\leq \varphi(0) + \rho\alpha\varphi'(0) \\
    \varphi(\alpha) &\geq \varphi(0) + (1 - \rho)\alpha\varphi'(0)
\end{align}
where$\rho \in (\frac{1}{2}, 1)$ is a fixed parameter.
\par
However, the downside of Goldstein Conditions
is that the optimal value might not lie in
the valid area.
\par\noindent\textit{Wolfe-Powell Conditions}
\begin{align}
    \varphi(\alpha) &\leq \varphi(0) + \rho\alpha\varphi'(0) \\
    \varphi'(\alpha) &\geq \sigma\varphi'(0)
\end{align}
where $\sigma \in (\rho, 1)$.

\subsection{Global Convergence}
\begin{theorem}
    Assume $\bigtriangledown f(x)$ exists and uniformly continuous
    on level set $L(x^{(0)}) = \{ x | f(x) \leq f(x^{(0)})\}$.
    Denote $\theta^{(k)}$ as the angle between $d^{(k)}$
    and $-\bigtriangledown f(x^{(k)})$.
    \begin{align}
        \theta^{(k)} \leq \frac{\pi}{2} - \mu
    \end{align}
    If step factor is determined by following methods
    \begin{itemize}
        \item Exace Line Search
        \item Goldstein Conditions
        \item Wolfe-Powell Conditions
    \end{itemize}
    Then, there exists $k$, such that $\bigtriangledown f(x^{(k)}) = 0$,
    or $f(x^{(k)}) \rightarrow 0$ or $f(x^{(k)}) \rightarrow - \infty$.
\end{theorem}
\begin{proof}
    
\end{proof}

\subsection{Steepest Descent Method}
Steepest Descent Method is a Line Search Method.
\begin{align}
    x^{(k+1)} = x^{(k)} - \alpha_k \bigtriangledown f(x^{(k)})
\end{align}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Termination error $\epsilon$, cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, k:= 0$\;
     \While{$\parallel g^{(k)} \parallel \geq \epsilon$}{
        $d^{(k)} = - g^{(k)}$\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$, $k:= k+1$\;
        Compute $g^{(k)} = \bigtriangledown f(x^{(k)})$
     }
     \caption{Steepest Descent Algorithm}
\end{algorithm}
Steepest Descent Method has linear convergence rate generally.

\subsection{Newton Method}

Newton Method is also a Line Search Method.
\begin{align}
    f(x^{(k)} + s) \approx q^{(k)}(s) f(x^{(k)}) + g^{(k)^T} s
     + \frac{1}{2} s^T G_k s
\end{align}
where $g^{(k)} = \bigtriangledown f(x^{(k)})$,
$G_k = \bigtriangledown^2 f(x^{(k)})$.
To minimize $q^{(k)}(s)$, we have
\begin{align}
    s = G_k^{-1} g^{(k)}
\end{align}
Notice that $G_k^{-1} g^{(k)}$ is the Newton Direction.
\par
\vspace{.3em}
\noindent\textit{Analysis on quadratic function}
\par
For positive definite quadratic function
\begin{align}
    f(x) = \frac{1}{2} x^TGx - c^Tx
\end{align}
In this case, $\bigtriangledown^2 f(x) = G$.
Let $H_0 = G^{-1}$, then we have
\begin{align}
    d^{(0)} &= H_0 \bigtriangledown f(x^{(0)}) \\
    &= G^{-1} (Gx^{(0)} - c) \\
    &= x^{(0)} - G^{-1}c \\
    &= x^{(0)} - x^*
\end{align}
So that Newton Method can reach global optimal
in $1$ iteration for quadratic functions.
\par
For general non-linear functions, if we follow
\begin{align}
    x^{(k+1)} = x^{(k)} - G_k^{-1} g^{(k)}
\end{align}
we called it Newton Method.
\par
\vspace{.3em}
\noindent\textit{Convergence Rate of Newton Method}
\begin{theorem}
    $f \in \mathcal{C}^2$, $x^{(k)}$ is sufficiently closed
    to optimal point $x^*$, where $\bigtriangledown f(x^*) = 0$.
    If $\bigtriangledown^2 f(x^*)$ is positive definite, 
    Hesse matrix of $f$ satisfies Lipschitz Condition, i.e.,
    $\exists \beta >0$, such that for all $(i, j)$,
    \begin{align}
        | G_{ij}(x) - G_{ij}(y)| \leq \beta \parallel x - y \parallel
    \end{align}
    Then $\{x^{(k)}\} \rightarrow x^*$, and have quadratic
    convergence rate.
\end{theorem}
\begin{proof}
    Denote $g(x) = \bigtriangledown f(x)$, then we have
    \begin{align}
        g(x - h) = g(x) - G(x)h + O(\parallel h \parallel^2)
    \end{align}
    Let $x = x^{(k)}$, $h = h^{(k)} = x^{(k)} - x^*$, then
    \begin{align}
        g(x^*) = g(x^{(k)}) - G(x^{(k)})(h^{(k)})
        + O(\parallel h^{(k)} \parallel^2) = 0
    \end{align}
    From Lipschitz Condition, we can easily get 
    $G(x^{(k)})^{-1}$ is finite. Then we left multiply
    $G(x^{(k)})^{-1}$ to Equation (66)
    \begin{align}
        0 &= G(x^{(k)})^{-1} g(x^{(k)}) - h^{(k)}
         + O(\parallel h^{(k)} \parallel^2) \\
        &= x^* - x^{(k)} + G(x^{(k)})^{-1} g(x^{(k)})
         + O(\parallel h^{(k)} \parallel^2) \\
         &= x^* - x^{(k+1)}
         + O(\parallel h^{(k)} \parallel^2) \\
         &= - h^{(k+1)} + O(\parallel h^{(k)} \parallel^2)
    \end{align}
    i.e.,
    \begin{align}
        \parallel h^{(k+1)} \parallel = O(\parallel h^{(k)} \parallel^2)
    \end{align}
\end{proof}

\subsection{Quasi-Newton Methods}
Newton Method has a fast convergence rate.
However, Newton Method requires second-order derivative,
if Hesse matrix is not positive definite, Newton Method
might not work well.
\par
In order to overcome the above difficulties,
Quasi-Newton Method is introduced.
Its basic idea is that:
Using second-order derivative free matrix $H_k$
to approximate $G(x^{(k)})^{-1}$.
Denote $s^{(k)} = x^{(k+1)} - x^{(k)}$,
$y^{(k)} = \bigtriangledown f(x^{(k+1)}) - \bigtriangledown f(x^{(k)})$,
then we have
\begin{align}
    \bigtriangledown^2 f(x^{(k)}) s^{(k)} \approx y^{(k)}
\end{align}
or
\begin{align}
    \bigtriangledown^2 f(x^{(k)})^{-1} y^{(k)} \approx s^{(k)}
\end{align}
So we need to construct $H_{k+1}$ such that
\begin{align}
    H_{k+1} y^{(k)} \approx s^{(k)}
\end{align}
or
\begin{align}
    y^{(k)} \approx B_{k+1} s^{(k)}
\end{align}
we called (74), (75) \textit{Quasi-Newton Conditions}
or \textit{Secant Conditions}.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, H_0 = I, k:= 0$\;
     \While{some conditions}{
        $d^{(k)} = - H_kg^{(k)}$\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$\;
        generate $H_{k+1}$, $k:= k+1$
     }
     \caption{Quasi-Newton Algorithm}
\end{algorithm}
\vspace{.3em}
\noindent\textbf{How to generate $H_k$}
\par
$H_k$ is the approximation matrix in $k$th iteration, we want to
generate $H_{k+1}$ from $H_k$
\par
\vspace{.3em}
\noindent\textit{Symmetric Rank 1}
\par
Assume
\begin{align}
    H_{k+1} = H_k + a \mathbf{u}\mathbf{u}^T,
     \quad a \in \mathbb{R}, \mathbf{u} \in \mathbb{R}^n
\end{align}
From the Quasi-Newton Conditions, we have
\begin{align}
    H_{k+1} \mathbf{y}^{(k)} &= \mathbf{s}^{(k)} \\
    H_k \mathbf{y}^{(k)} + a \mathbf{u}\mathbf{u}^T \mathbf{y}^{(k)}
    &= \mathbf{s}^{(k)} \\
    H_k \mathbf{y}^{(k)} + a \mathbf{u}^T\mathbf{y}^{(k)} \mathbf{u} 
    &= \mathbf{s}^{(k)}
\end{align}
Let $\mathbf{u} = \mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)}$, 
$a = \frac{1}{\mathbf{u}^T \mathbf{y}}$,
clearly this is a solution of the equation.
Here we have
\begin{align}
    H_{k+1} = \frac{(\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})
    (\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})^T}
    {(\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})^T \mathbf{y}^{(k)}}
\end{align}
(79) is \textit{Symmetric Rank 1 Update}.
The problem of Symmetric Rank 1 Update is that
the positive-definite property of $H_k$
can not be preserved.

\vspace{.3em}
\par
\noindent\textit{Symmetric Rank 2 Update}
\par
Assume
\begin{align}
    H_{k+1} = H_k + a \mathbf{u}\mathbf{u}^T +
     b \mathbf{v}\mathbf{v}^T,
     \quad a, b \in \mathbb{R}, \mathbf{u}, \mathbf{v} \in \mathbb{R}^n
\end{align}
such that Quasi-Newton Conditions stand.
We can find a solution of $a, b, \mathbf{u}, \mathbf{v}$ that is
\begin{align}
    \left\{
    \begin{array}{ll}
        \mathbf{u} = \mathbf{s}^{(k)}, 
        \quad a \mathbf{u}^T \mathbf{y} = 1 \\
        \mathbf{v} = H_k \mathbf{y}^{(k)}, 
        \quad b \mathbf{v}^T \mathbf{y} = -1
    \end{array}
    \right.
\end{align}
So that we have
\begin{align}
    H_{k+1} = H_k + \frac{\mathbf{s}^{(k)}\mathbf{s}^{(k)T}}
    {\mathbf{s}^{(k)T} \mathbf{y}^{(k)}} - 
    \frac{H_k\mathbf{y}^{(k)} \mathbf{y}^{(k)T} H_k}
    {\mathbf{y}^{(k)T}H_k\mathbf{y}^{(k)}}
\end{align}
We called (83) the DFP (Davidon-Fletcher-Powell) update.
\par
From Quasi-Newton Condition (75), we can get the
BFGS (Broyden-Fletcher-Goldfarb-Shanno) update
\begin{align}
    B_{k+1}^{(BFGS)} = B_k +
    \frac{\mathbf{y}^{(k)}\mathbf{y}^{(k)T}}
    {\mathbf{y}^{(k)T} \mathbf{s}^{(k)}}
    - \frac{B_k \mathbf{s}^{(k)}\mathbf{s}^{(k)T} B_k}
    {\mathbf{s}^{(k)T} B_k \mathbf{s}^{(k)}}
\end{align}
\vspace{.3em}
\par\noindent\textit{Inverse of SR1 update}
\begin{theorem}[Sherman-Morrison]
    $A \in \mathbb{R}^n \times \mathbb{R}^n$ is a 
    non-singular matrix, $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$.
    If $1 + \mathbf{v}^TA^{-1}\mathbf{u} \neq 0$,
    then SR1 update of $A$ is non-singular, and its inverse
    can be represented as
    \begin{align}
        (A + a\mathbf{u}\mathbf{v}^T)^{-1} = 
        A^{-1} - \frac{A^{-1}\mathbf{u}\mathbf{v}^TA^{-1}}
        {1 + \mathbf{v}^TA^{-1}\mathbf{u}}
    \end{align}
\end{theorem}

\subsection{Conjugate Gradient Method}
\begin{definition}{Conjugate Direction.}
    $G$ is a $n \times n$ positive definite matrix, 
    \par\noindent for non-zero vector set
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\} \in \mathbb{R}^n$,
    if $\mathbf{d}^{(i)T} G \mathbf{d}^{(j)} = 0, (i \neq j)$,
    then we called $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$
    is \emph{G-Conjugate}.
    \label{def:Conjugate}
\end{definition}

\begin{lemma}
    For non-zero conjugate vector set 
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\} \in \mathbb{R}^n$,
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ are linearly independent.
\end{lemma}
\begin{proof}
    From Definition \ref{def:Conjugate}, we have
    \begin{align}
        \mathbf{d}^{(i)T} G \mathbf{d}^{(j)} = 0, \forall i, j, i \neq j
    \end{align}
    if $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ is linearly dependent,
    there exists 
    \begin{align}
        \mathbf{d}^{(t)} = \sum_{j=0}^k c_j \mathbf{d}^{(j)}
    \end{align} 
    then
    \begin{align}
        \mathbf{d}^{(t)T} G \mathbf{d}^{(i)} = 
        \sum_{j=0}^k c_j \mathbf{d}^{(j)} G \mathbf{d}^{(i)}
        = c_i \mathbf{d}^{(i)} G \mathbf{d}^{(i)} \neq 0
    \end{align}
    so that $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$
    are linearly independent.
    \qed
    \label{lemma:Conjugate}
\end{proof}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n$, positive definite matrix $G$, $k:= 0$\;
     Construct $\mathbf{d}^{(0)}$ such that
     $\mathbf{g}^{(0)T}\mathbf{d}^{(0)} < 0$\;
     \While{some conditions}{
        solve $\min_{\alpha_k \geq 0} f(\mathbf{x}^{(k)} + \alpha_k \mathbf{d}^{(k)})$\;
        $\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k\mathbf{d}^{(k)}$\;
        Construct $\mathbf{d}^{(k+1)}$ such that 
        $\mathbf{d}^{(k+1)}G\mathbf{d}^{(j)} = 0, j = 0,..., k$.\;
        $k:=k+1$
     }
     \caption{Conjuagte Gradient Algorithm}
\end{algorithm}

\begin{theorem}[Conjugate Gradient]
    For strictly convex quadratic function 
    \par\noindent
    $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T G \mathbf{x} + \mathbf{c}^T\mathbf{x}$,
    apply conjugate gradient method combined with exact line search,
    then $\mathbf{x}^{(k+1)}$ is the global minima in manifold
    \begin{align}
        \mathcal{V} =  \{ \mathbf{x} | \mathbf{x} = \mathbf{x}^{(0)} + 
        \sum_{j=0}^k\beta_j\mathbf{d}^{(j)}, \forall \beta_j \in \mathbb{R} \}
    \end{align}
    \label{th:Conjugate}
\end{theorem}
\begin{proof}
    Firstly, from Lemma \ref{lemma:Conjugate}, we have 
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ are linearly independent.
    So we only need to prove that 
    for all $k < n$
    \begin{align}
        \mathbf{g}^{(k+1)T}\mathbf{d}^{(j)} = 0, j = 0,..., k
    \end{align}
    i.e., $\mathbf{g}^{(k+1)}$ is orthogonal with subspace
    $span\{\mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$.
    \par
    Due to the exact line search, $\forall j$
    \begin{align}
        \mathbf{g}^{(j+1)T}\mathbf{d}^{(j)} = 0
    \end{align}
    especially $\mathbf{g}^{(k+1)T}\mathbf{d}^{(k)} = 0$.
    \par
    Notice that 
    \begin{align}
        \mathbf{g}^{(k+1)} - \mathbf{g}^{(k)}
        = G (\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)})
        = \alpha_k G \mathbf{d}^{(k)}
    \end{align}
    so that we have $\forall j \leq k$
    \begin{align}
        \mathbf{g}^{(k+1)T}\mathbf{d}^{(j)}
        &= (\sum_{m=j+1}^k (\mathbf{g}^{(m+1)T} - \mathbf{g}^{(m)T})
        + \mathbf{g}^{(j+1)T} )\mathbf{d}^{(j)} \\
        &= \sum_{m=j+1} \alpha_m \mathbf{d}^{(m)T}G \mathbf{d}^{(j)}
        + \mathbf{g}^{(j+1)T} \mathbf{d}^{(j)} \\
        &= 0
    \end{align}
    \qed
\end{proof}
\begin{lemma}
    For strictly convex quadratic function 
    $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T G \mathbf{x} + \mathbf{c}^T\mathbf{x}$,
    apply conjugate gradient method combined with exact line search,
    $\mathbf{g}(\mathbf{x}) = \bigtriangledown f(\mathbf{x}) = G\mathbf{x} + \mathbf{c}$,
    we have
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)} = 0, \forall j = 0,..., k-1
    \end{align}
    \label{lemma:Conjugate}
\end{lemma}
\begin{proof}
    From Theorem \ref{th:Conjugate}, we have
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)}
        = \mathbf{g}^{(k)T} (-\mathbf{d}^{(j)} 
        + \sum_{i=0}^{j-1}\beta_i^{(j)} \mathbf{d}^{(i)}) = 0
    \end{align}
    \qed
\end{proof}
\noindent\textbf{Quadratic function case}
\par
For $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T G \mathbf{x} + \mathbf{c}^T x$,
$G$ is a $n \times n$ positive definite matrix.
\begin{align}
    \mathbf{g}(\mathbf{x}) = G \mathbf{x} + \mathbf{c}
\end{align}
Set $\mathbf{d}^{(0)} = -\mathbf{g}^{(0)}$,
exact line search for $\alpha_0$ such that 
$\mathbf{g}^{(1)T}\mathbf{d}^{(0)} = 0$.
Assume $\mathbf{d}^{(1)} = -\mathbf{g}^{(1)} + \beta_0^{(1)}\mathbf{d}^{(0)}$,
select $\beta_0^{(1)}$ such that
$ \mathbf{d}^{(1)} G \mathbf{d}^{(0)} = 0$
\begin{align}
    \beta_0^{(1)} = \frac{\mathbf{g}^{(1)T}\mathbf{g}^{(1)}}
    {\mathbf{g}^{(0)T}\mathbf{g}^{(0)}}
\end{align}
\begin{proof}
    From (92), we have
    \begin{align}
        &\mathbf{d}^{(1)T} G \mathbf{d}^{(0)} = 0 \\
        \Leftrightarrow \quad&\mathbf{d}^{(1)T}
        (\mathbf{g}^{(1)} - \mathbf{g}^{(0)}) = 0 \\
        \Leftrightarrow  \quad&(\mathbf{g}^{(1)} + 
        \beta_0^{(1)}\mathbf{g}^{(0)})^T
        (\mathbf{g}^{(1)} - \mathbf{g}^{(0)}) = 0 \\
        \Leftrightarrow \quad&\mathbf{g}^{(1)T}\mathbf{g}^{(1)}
        - \beta_0^{(1)}\mathbf{g}^{(0)T}\mathbf{g}^{(0)} = 0 \\
        \Leftrightarrow \quad&\beta_0^{(1)} = 
        \frac{\mathbf{g}^{(1)T}\mathbf{g}^{(1)}}
        {\mathbf{g}^{(0)T}\mathbf{g}^{(0)}}
    \end{align}
    \qed
\end{proof}
    Generally, we can select $\beta_j^{(k)}$ such that
    $\mathbf{d}^{(k)T} G \mathbf{d}^{(j)} = 0, j=0, 1,..., k-1$
    that is
    \begin{align}
        \mathbf{d}^{(k)T} G \mathbf{d}^{(j)} &= 0 \\
        (-\mathbf{g}^{(k)T} + \sum_{i=0}^{k-1} \beta_i^{(k)} \mathbf{d}^{(i)T})
        G \mathbf{d}^{(j)} &= 0 \\
        -\mathbf{g}^{(k)T} G \mathbf{d}^{(j)}
        + \beta_j^{(k)}\mathbf{d}^{(j)T} G \mathbf{d}^{(j)} &= 0
    \end{align}
    so we have
    \begin{align}
        \beta_j^{(k)} = \frac{\mathbf{g}^{(k)T} G \mathbf{d}^{(j)}}
        {\mathbf{d}^{(j)T} G \mathbf{d}^{(j)}}
        = \frac{\mathbf{g}^{(k)T} (\mathbf{g}^{(j+1)} - \mathbf{g}^{(j)})}
        {\mathbf{d}^{(j)T}(\mathbf{g}^{(j+1)} - \mathbf{g}^{(j)})}
    \end{align}
    From Lemma \ref{lemma:Conjugate}, we have 
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)} = 0, \forall j = 0,..., k-1
    \end{align}
    So
    \begin{align}
        \beta_j^{(k)} &= 0, j = 0,...,k-2 \\
        \beta_{k-1}^{(k)} &= \frac{\mathbf{g}^{(k)T} (\mathbf{g}^{(k)}}{\mathbf{g}^{(k-1)T} (\mathbf{g}^{(k-1)}}
    \end{align}

\subsection{Trust Region Method}
Previously, we use a direction search strategy
to determine a search direction, then use line
search method to determine step length.
\par
Now we discuss a new global convergence strategy --
Trust-Region Method.
\begin{definition}[Trust Region]
    \begin{align}
        \Omega_k = \{ \mathbf{x} \in \mathbb{R}^n \ | \ \parallel
        \mathbf{x} - \mathbf{x}^{(k)} \parallel \leq e_k  \}
    \end{align}
    We called $\Omega_k$ \emph{Trust Region}, $e_k$ is the \emph{Trust radius}.
\end{definition}

Suppose in this neighborhood, quadratic model $q^{(k)}(\mathbf{s})$
is a proper approximation of $f(\mathbf{x})$.
We minimize the quadratic model in trust region, derive
approximate minima $\mathbf{s}^{(k)}$,
and set $\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \mathbf{s}^{(k)}$.
\par
\noindent\textbf{Trust Region Subproblem}
\begin{align}
    \min_{\parallel \mathbf{s} \parallel \leq e_k} q^{(k)}(\mathbf{s})
     = f(\mathbf{x}^{(k)}) + \mathbf{g}^{(k)T}\mathbf{s}
    + \frac{1}{2} \mathbf{s}^T B_k \mathbf{s}
\end{align}

Where $\mathbf{s} = \mathbf{x} - \mathbf{x}^{(k)}$,
$\mathbf{g}^{(k)} = \bigtriangledown f(\mathbf{x}^{(k)})$,
$B_k = \bigtriangledown^2 f(\mathbf{x}^{(k)})$.
$e_k$ is the trust region radius.

\par
\noindent\textbf{How to select $e_k$}
\par
Denote the solution of the subproblem as $\mathbf{s}^{(k)}$,
then let
\begin{align}
    \textnormal{Act}_k = f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k)} + \mathbf{s}^{(k)}) \\
    \textnormal{Pre}_k = q^{(k)}(\mathbf{0}) - q^{(k)}(\mathbf{s}^{(k)}) 
\end{align}

Define
\begin{align}
    r_k = \frac{\textnormal{Act}_k}{\textnormal{Pre}_k}
    = \frac{f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k)} + \mathbf{s}^{(k)})}
    {q^{(k)}(\mathbf{0}) - q^{(k)}(\mathbf{s}^{(k)})}
\end{align}

to measure the difference between objective function
and the quadratic approximate model.
\par
We can update $e_k$ according to $r_k$. If $r_k$ is too small,
that means our model can not fit the objective function well,
so we need to decrease $e_k$. If $r_k$ is close to $1$,
that means out model is good and we can increase $r_k$.
Set the parameters $0 < \gamma_1 < \gamma_2 < 1$ and 
$0 < \eta_1 < 1 < \eta_2$, we can have the following update
rule
\begin{align}
    e_{k+1} = \left\{
        \begin{array}{lll}
            &\eta_1 e_k &\textnormal{if} \ r_k < \gamma_1 \\
            &e_k &\textnormal{if} \ \gamma_1 < r_k < \gamma_2 \\
            &\min(\eta_2 e_k, \bar{e}) \quad &\textnormal{if} \ r_k \geq \gamma_2
        \end{array}\right.
\end{align}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n$, $e_0 \in (0, \bar{e})$, $\epsilon > 0$,
     $0 < \gamma_1 < \gamma_2 < 1$, $0 < \eta_1 < 1 < \eta_2$, $k:= 0$\;
     \While{$\parallel \mathbf{g}^{(k)} \parallel \geq \epsilon$}{
        solve the subproblem to derive $\mathbf{s}^{(k)}$\;
        calculate $r_k$, update $\mathbf{x}$\;
        \eIf{$r_k > 0$}
        {$\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \mathbf{s}^{(k)}$}
        {$\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)}$}
        update $e_k$ following (117)\;
        $k:=k+1$\;
     }
     \caption{Trust Region Algorithm}
\end{algorithm}










\clearpage
\section{Constrained Optimization}
\subsection{Quadratic Programming}

\begin{align}
    \min \ Q(\mathbf{x}) &= \frac{1}{2}\mathbf{x}^T G\mathbf{x} + \mathbf{c}^T \mathbf{x}) \\
    s.t. \ \ \mathbf{a}_i^T \mathbf{x} &= b_i, i \in \mathcal{E} = \{1,...,m_e \} \\
    \mathbf{a}_i^T \mathbf{x} &\geq b_i, i \in \mathcal{I} = \{m_e+1,...,m \}
\end{align}

We assume that $G$ is a symmetric matrix and $\mathbf{a}_i, i \in \mathcal{E}$
be linearly independent.

\par
\noindent\textbf{Solution of Quadratic Programming}
\par
If $G$ be positive semi-definite matrix, the 
Quadratic Programming problem is a convex optimization
problem, so any of its local minima is a global minima.
\par
If $G$ be positive definite matrix, the
solution to the Quadratic Programming problem
is unique, if exists.
\par
If $G$ be indefinite, there is no guarantee to the solution.

\par
\noindent\textbf{Equality Constrained Quadratic Programming}
\begin{align}
    \min \ Q(\mathbf{x}) &= \frac{1}{2}\mathbf{x}^T G\mathbf{x} + \mathbf{c}^T \mathbf{x} \\
    s.t. \ \ A \mathbf{x} &= \mathbf{b}
\end{align}

\par
\noindent\textbf{General Quadratic Programming}
\begin{align}
    \min \ Q(\mathbf{x}) &= \frac{1}{2}\mathbf{x}^T G\mathbf{x} + \mathbf{c}^T \mathbf{x}) \\
    s.t. \ \ \mathbf{a}_i^T \mathbf{x} &= b_i, i \in \mathcal{E} = \{1,...,m_e \} \\
    \mathbf{a}_i^T \mathbf{x} &\geq b_i, i \in \mathcal{I} = \{m_e+1,...,m \}
\end{align}

The idea is to remove or transform the inequality constraints.
If the inequality constraint is not active near the solution,
we can ignore the constraint; For the active inequality constraints,
we can use equality constraints to replace them.

\begin{theorem}[Active Set]
    Denote $\mathbf{x}^*$ as a local minima of general quadratic
    problem \emph{(123)}, then $\mathbf{x}^*$ must be a local minima of
    the equality constrained problem
    \begin{align}
        (\textnormal{EQ})\left\{
            \begin{array}{ll}
                \min \ Q(\mathbf{x}) &= \frac{1}{2} \mathbf{x}^T G\mathbf{x}
                + \mathbf{c}^T\mathbf{x} \\
                s.t. \ \ \mathbf{a}_i^T\mathbf{x} &= b_i, i \in \mathcal{E}
                \cup \mathcal{I}(\mathbf{x}^*)
            \end{array}
        \right.
    \end{align}
    Meanwhile, if $\mathbf{x}^*$ is a feasible point of \emph{(123)},
    and the K-T point of (EQ), $\lambda^* \geq 0, i \in \mathcal{I}(\mathbf{x}^*)$,
    then $\mathbf{x}^*$ must be the K-T point of \emph{(123)}.
\end{theorem}
\begin{proof}
    Recall the K-T condition, we can get that
    there exists $\lambda_i \geq 0, i \in I(\mathbf{x}^*)$ and $\mu_j$ s.t.
    \begin{align}
        \bigtriangledown Q(\mathbf{x}^*) - \sum_{i\in I(\mathbf{x}^*)} \lambda_i \mathbf{a}_i
        - \sum_{j \in \mathcal{E}} \mu_j \mathbf{a}_j = 0
    \end{align}
    the K-T condition of (EQ) is there exists $\lambda_i, i \in \mathcal{E} \cup \mathcal{I}(\mathbf{x}^*)$,
    s.t.
    \begin{align}
        \bigtriangledown Q(\mathbf{x}^*)
        - \sum_{j \in \mathcal{E} \cup \mathcal{I}(\mathbf{x}^*)} \lambda_j \mathbf{a}_j = 0
    \end{align}
    Appearently If $\mathbf{x}^*$ satisfies (127), then it also satisfies (128).
    On the other hand, if $\mathbf{x}^*$ satisfies (128) and $\lambda_i \geq 0, i \in I(\mathbf{x}^*)$,
    we have
    \begin{align}
        &\bigtriangledown Q(\mathbf{x}^*)
        - \sum_{j \in \mathcal{E} \cup \mathcal{I}(\mathbf{x}^*)} \lambda_j \mathbf{a}_j = 0 \\
        \Leftrightarrow \ &
        \bigtriangledown Q(\mathbf{x}^*) - \sum_{i\in I(\mathbf{x}^*)} \lambda_i \mathbf{a}_i
        - \sum_{j \in \mathcal{E}} \lambda_j \mathbf{a}_j = 0
    \end{align}
    i.e., $\mathbf{x}^*$ satisfies (127).
    \qed
\end{proof}

\subsection{Non-linear Constrained Optimization}
\noindent\textbf{Equality Constrained Problem}
\par
\emph{Lagrange-Newton method}
\par
\begin{align}
    \min \ &f(\mathbf{x}) \\
    s.t. \ &\mathbf{c}(\mathbf{x}) = \mathbf{0}
\end{align}
where $\mathbf{c}(\mathbf{x}) = (c_1(\mathbf{x}),..., c_m(\mathbf{x}))^T$.
\par
Denote $A(\mathbf{x}) = [\bigtriangledown\mathbf{c}(\mathbf{x})]^T 
= (\bigtriangledown c_1(\mathbf{x}),..., \bigtriangledown c_m(\mathbf{x}))^T$.
The K-T condition of the problem is there exists
$\mathbf{\lambda} \in \mathbb{R}^m$ s.t.
\begin{align}
    \bigtriangledown f(\mathbf{x}) - A(\mathbf{x})^T \mathbf{\lambda} = \mathbf{0}
\end{align}
and $\mathbf{c}(\mathbf{x}) = \mathbf{0}$.
\par
We can use Newton-Raphson method to solve the equations by
\begin{align}
    \left(\begin{array}{ll}
        W(\mathbf{x}, \mathbf{\lambda}) & -A(\mathbf{x})^T \\
        -A(\mathbf{x}) & 0
    \end{array}\right)
    \left(\begin{array}{ll}
        \delta_{x} \\
        \delta_\lambda
    \end{array}\right) = -
    \left(
        \begin{array}{ll}
            \bigtriangledown f(\mathbf{x}) - A(\mathbf{x})^T \mathbf{\lambda} \\
            \mathbf{c}(\mathbf{x})
        \end{array}\right)
\end{align}
where $W(\mathbf{x}, \mathbf{\lambda}) = \bigtriangledown^2 f(\mathbf{x})
- \sum_{i=1}^m \lambda_i \bigtriangledown^2 c_i(\mathbf{x})$.
\par
We called the method above as \emph{Lagrange-Newton Method}.
\par
Here we can define 
\begin{align}
    \psi(\mathbf{x}, \lambda) = \parallel \bigtriangledown f(\mathbf{x})
    - A(\mathbf{x})^T\lambda \parallel^2 + \parallel\mathbf{c}(\mathbf{x})
    \parallel^2
\end{align}
so that $\psi$ is a descent function to Lagrange-Newton method.
\begin{align}
    \bigtriangledown \psi(\mathbf{x}, \lambda)^T
    \left(\begin{array}{ll}
        \delta_{x} \\
        \delta_\lambda
    \end{array}\right) = 
    -2 \psi(\mathbf{x}, \lambda) \neq 0
\end{align}
\par
\emph{Sequential Quadratic Programming method}
\par
(134) can be rewritten into
\begin{align}
    \left\{
        \begin{array}{ll}
            W(\mathbf{x}, \lambda) \delta_x + \bigtriangledown f(\mathbf{x})
            &= A(\mathbf{x})^T(\lambda + \delta_\lambda) \\
            \mathbf{c}(\mathbf{x}) + A(\mathbf{x})\delta_x &= \mathbf{0}
        \end{array}
        \right.
\end{align}
From K-T condition, we notice that $\delta_x$ is the
K-T point of the following Quadratic Programming problem
\begin{align}
    \min \ &\frac{1}{2} \mathbf{d}^T W(\mathbf{x}, \lambda) \mathbf{d}
    + \bigtriangledown f(\mathbf{x})^T\mathbf{d} \\
    s.t. \ &\mathbf{c}(\mathbf{x}) + A(\mathbf{x})\mathbf{d} = 0
\end{align}

So we can solve a Quadratic Programming subproblem to
derive $\delta_x$, we called this method
\emph{Sequential Quadratic Programming}.

\par
\noindent\textbf{General Nonlinear Constrained Problem}
\par
\emph{Sequential Quadratic Programming method}
\par
\begin{align}
    \min \ &f(\mathbf{x}) \\
    s.t. \ &c_i(\mathbf{x}) = 0, \quad i \in \mathcal{E} = \{1,...,m_e\} \\
    &c_i(\mathbf{x}) \geq 0, \quad i \in \mathcal{I} = \{m_e+1,...,m\}
\end{align}
Similarly, we can construct subproblem
\begin{align}
    \min \ &\frac{1}{2}\mathbf{d}^TW\mathbf{d} + \mathbf{g}^T\mathbf{d} \\
    s.t. \ &c_i(\mathbf{x}) + \mathbf{a}_i(\mathbf{x})^T\mathbf{d} = 0, i \in \mathcal{E} \\
     &c_i(\mathbf{x}) + \mathbf{a}_i(\mathbf{x})^T\mathbf{d} \geq 0, i \in \mathcal{I}
\end{align}
Here, $W$ is the Hesse matrix (or its approximation) of the Lagrange
function of (140), $\mathbf{g} = \bigtriangledown f(\mathbf{x})$,
$A(\mathbf{x}) = (\mathbf{a}_1(\mathbf{x}),...,\mathbf{a}_m(\mathbf{x})$.

Denote the solution to subproblem (143) as $\mathbf{d}$,
the corresponding Lagrange multiplier vector $\bar{\lambda}$,
so we have
\begin{align}
    \left\{
        \begin{array}{llll}
            &W\mathbf{d} + \mathbf{g} = A(\mathbf{x})^T\bar{\lambda} \\
            &\bar{\lambda}_i \geq 0, i \in \mathcal{I} \\
            &\mathbf{c}(\mathbf{x}) + A(\mathbf{x})\mathbf{d} = 0, i \in \mathcal{E} \\
            &\mathbf{c}(\mathbf{x}) + A(\mathbf{x})\mathbf{d} \geq 0, i \in \mathcal{I}
        \end{array}\right.
\end{align}

\par
\emph{Penalty method}
\par
For nonlinear constrained porblem (140),
we can use objective function $f(\mathbf{x})$
and constraint function $\mathbf{c}(\mathbf{x})$
to construct \emph{Penalty function}
\begin{align}
    P(\mathbf{x}) = P(f(\mathbf{x}), \mathbf{c}(\mathbf{x}))
\end{align}

We need the penalty function have the property that:
for feasible points, $P(\mathbf{x}) = f(\mathbf{x})$,
otherwise, $P(\mathbf{x}) > f(\mathbf{x})$.
\par
To measure the destructiveness to the constraints,
we define $\mathbf{c}(\mathbf{x})\_$
\begin{align}
    \left\{
        \begin{array}{ll}
            c_i(\mathbf{x})\_ &= c_i(\mathbf{x}), \quad \quad \quad \quad \quad i\in \mathcal{E} \\
            c_i(\mathbf{x})\_ &= |\min \{ 0, c_i(\mathbf{x}) \}|,\ \ i \in \mathcal{I}
        \end{array}\right.
\end{align}

Consider simple penalty function
\begin{align}
    P_\sigma(\mathbf{x}) = f(\mathbf{x}) + 
    \sigma \parallel \mathbf{c}(\mathbf{x})\_ \parallel^2
\end{align}
Denote $\mathbf{x}(\sigma)$ as the solution to unconstrained
problem $\min P_\sigma(\mathbf{x})$, we have the following lemma:

\begin{lemma}[Penalty method]
    If $\mathbf{x}(\sigma)$ is a feasible point of nonlinear
    constrained problem \textnormal{(140)}, then $\mathbf{x}(\sigma)$
    aslo is the solution to \textnormal{(140)}.
\end{lemma}

\begin{proof}
    From the definition of penalty function,
    we have $P(\mathbf{x}) = f(\mathbf{x})$,
    $\mathbf{x} \in \mathcal{S}$.
    If $\mathbf{x}(\sigma)$ is the solution to 
    $\min P(\mathbf{x})$, i.e.,
    \begin{align}
        P(\mathbf{x}(\sigma)) &\leq P(\mathbf{x}_0), \ \forall
        \mathbf{x}_0 \in \mathbb{R}^{n} \\
        f(\mathbf{x}(\sigma)) &\leq f(\mathbf{x}_0), \ \forall
        \mathbf{x}_0 \in \mathcal{S}
    \end{align}
that is, $\mathbf{x}(\sigma)$ is the solution to (140).
\qed
\end{proof}

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwInOut{Return}{return}
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n$, $\sigma_0 > 0$, $\beta > 1$,
     $\epsilon > 0$, $k:= 0$\;
     \While{$\parallel \mathbf{c}(\mathbf{x}(\sigma_{k-1}))\_ \parallel \geq \epsilon$}{
        solve the subproblem $\min_{\mathbf{x}\in \mathbb{R}^n} P_{\sigma_k}(\mathbf{x})$
        to get the solution $\mathbf{x}(\sigma_k)$\;
        $\mathbf{x}^{(k+1)} = \mathbf{x}(\sigma_k)$,
        $\sigma_{k+1} = \beta \sigma_k$\;
        $k:=k+1$\;
     }
     \Return{$\mathbf{x}(\sigma_{k-1})$}
     \caption{Penalty Method Algorithm}
\end{algorithm}

\begin{theorem}[Convergence of Penalty method]
    If $\epsilon > \min_{\mathbf{x} \in \mathbb{R}^n}
    \parallel \mathbf{c}(\mathbf{x})\_ \parallel$,
    then the algorithm can terminate in finite steps.
\end{theorem}

\begin{lemma}
    Let $\sigma_{k+1} > \sigma_k > 0$, then we have
    $P_{\sigma_k}(\mathbf{x}(\sigma_k)) \leq P_{\sigma_{k+1}}(\mathbf{x}(\sigma_{k+1}))$,
    $\parallel \mathbf{c}(\mathbf{x}(\sigma_k))\_ \parallel
    \geq \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ \parallel$,
    $f(\mathbf{x}(\sigma_k)) \leq f(\mathbf{x}(\sigma_{k+1}))$.
\end{lemma}
\begin{proof}
    \begin{align}
        P_{\sigma_{k+1}}(\mathbf{x}(\sigma_{k+1}))
        &= f(\mathbf{x}(\sigma_{k+1})) + 
        \sigma_{k+1} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ \parallel^2 \\
        &\geq f(\mathbf{x}(\sigma_{k+1})) + 
        \sigma_{k} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ \parallel^2 \\
        &\geq \min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x}) + 
        \sigma_{k} \parallel \mathbf{c}(\mathbf{x})\_ \parallel^2 \\
        &= P_{\sigma_k}(\mathbf{x}(\sigma_k))
    \end{align}
    From the definition, we have
    \begin{align}
        &f(\mathbf{x}(\sigma_{k})) + 
        \sigma_{k+1} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2 \\
        \geq 
        &f(\mathbf{x}(\sigma_{k+1})) + 
        \sigma_{k+1} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2 \\
        \geq 
        &f(\mathbf{x}(\sigma_{k+1})) + 
        \sigma_{k} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2 \\
        \geq 
        &f(\mathbf{x}(\sigma_{k})) + 
        \sigma_{k} \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2
    \end{align}
    From the inequalities above, we have
    \begin{align}
        &\sigma_{k} (\parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2 - \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2) \\
        \leq
        &f(\mathbf{x}(\sigma_{k+1})) - f(\mathbf{x}(\sigma_{k})) \\
        \leq 
        & \sigma_{k+1} (\parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2 - \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2)
    \end{align}
    So that
    \begin{align}
        \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel \geq \parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel
    \end{align}
    Then
    \begin{align}
        0\leq \sigma_{k} (\parallel \mathbf{c}(\mathbf{x}(\sigma_{k+1}))\_ 
        \parallel^2 - \parallel \mathbf{c}(\mathbf{x}(\sigma_{k}))\_ 
        \parallel^2)
        \leq
        f(\mathbf{x}(\sigma_{k+1})) - f(\mathbf{x}(\sigma_{k}))
    \end{align}
    i.e.,
    \begin{align}
        f(\mathbf{x}(\sigma_{k+1})) \geq f(\mathbf{x}(\sigma_{k}))
    \end{align}
    \qed
\end{proof}

\begin{lemma}
    Denote $\bar{\mathbf{x}}$ as the solution to
    problem \emph{(140)}, then for all
    $\sigma_k>0$,
    \begin{align}
        f(\bar{\mathbf{x}}) \geq
        P_{\sigma_k}(\mathbf{x}(\sigma_k))
        \geq f(\mathbf{x}(\sigma_k))
    \end{align}
\end{lemma}
\begin{proof}
    For all $\sigma_k>0$,
    \begin{align}
        f(\bar{\mathbf{x}}) &= 
        \min_{\mathbf{x} \in \mathbb{R}^n}
        \lim_{\sigma \rightarrow \infty}
        f(\mathbf{x}) + \sigma \parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2 \\
        &\geq \min_{\mathbf{x} \in \mathbb{R}^n}
        f(\mathbf{x}) + \sigma_k \parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2\\
        &= f(\mathbf{x}(\sigma_k)) + \sigma_k \parallel \mathbf{c}
        (\mathbf{x}(\sigma_{k}))\_ \parallel^2 \\
        &\geq f(\mathbf{x}(\sigma_k))
    \end{align}
    \qed
\end{proof}
\begin{lemma}
    Let $\delta = \parallel \mathbf{c}
    (\mathbf{x}(\sigma))\_ \parallel$,
    then $\mathbf{x}(\sigma)$ is also
    the solution to the problem
    \begin{align}
        \min \ &f(\mathbf{x}) \\
        s.t. \ &\parallel \mathbf{c}
        (\mathbf{x})\_ \parallel \leq \delta
    \end{align}
\end{lemma}
\begin{proof}
    The problem is equivalent to
    \begin{align}
        \min \ &f(\mathbf{x}) \\
        s.t. \ &\parallel \mathbf{c}
        (\mathbf{x})\_ \parallel \leq
        \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel
    \end{align}
    \begin{align}
        f(\mathbf{x}(\sigma)) + \sigma \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel^2 = 
        \min_{\mathbf{x} \in \mathbb{R}^n}
        f(\mathbf{x}) + \sigma \parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2
    \end{align}
    Then for all $\mathbf{x} \in \mathbb{R}^n$, we have
    \begin{align}
        f(\mathbf{x}(\sigma)) + \sigma \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel^2 &\leq 
        f(\mathbf{x}) + \sigma \parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2 \\
        f(\mathbf{x}(\sigma)) - f(\mathbf{x}) &\leq
        \sigma(\parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2 - \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel^2)
    \end{align}
    That is, if $\parallel \mathbf{c}
    (\mathbf{x})\_ \parallel \leq \parallel \mathbf{c}
    (\mathbf{x}(\sigma))\_ \parallel$, then
    \begin{align}
        f(\mathbf{x}(\sigma)) - f(\mathbf{x}) \leq
        \sigma(\parallel \mathbf{c}
        (\mathbf{x})\_ \parallel^2 - \parallel \mathbf{c}
        (\mathbf{x}(\sigma))\_ \parallel^2) \leq 0
    \end{align}
    i.e., for all $\mathbf{x} \in \mathbb{R}^n$,
    $f(\mathbf{x}(\sigma)) \leq f(\mathbf{x})$.
    \qed
\end{proof}





\clearpage
\section{Convex Optimization}
\subsection{Convex set}
\subsection{Convex function}
\subsection{Convex optimization}
\noindent\textbf{Quadratically constrained quadratic program}
\begin{align}
    \min \ &\frac{1}{2} \mathbf{x}^T P_0 \mathbf{x}
    + \mathbf{q}_0^T \mathbf{x} + r_0 \\
    s.t. \ &\frac{1}{2} \mathbf{x}^T P_i \mathbf{x}
    + \mathbf{q}_i^T \mathbf{x} + r_i \leq 0, \quad i = 1,...,m \\
    & A \mathbf{x} = \mathbf{b}
\end{align}

\par
\noindent\textbf{Second-order cone program}
\begin{align}
    \min \ & \mathbf{f}^T \mathbf{x} \\
    s.t. \ & \parallel A_i \mathbf{x} + \mathbf{b}_i \parallel
    \leq \mathbf{c}_i^T\mathbf{x} + \mathbf{d}_i, \quad i = 1,...,m \\
    & F\mathbf{x} = \mathbf{g}
\end{align}

\begin{lemma}
    Any QCQP problem can be formulated as a SOCP problem.
\end{lemma}
\begin{proof}
    The QCQP problem is equivalent to
    \begin{align}
        \min \ & - r_0 \\
        s.t. \ &\frac{1}{2} \mathbf{x}^T P_i \mathbf{x}
        + \mathbf{q}_i^T \mathbf{x} + r_i \leq 0, \quad i = 0,...,m \\
        & A \mathbf{x} = \mathbf{b}
    \end{align}
    Then we need to prove that (121) can be formulated as (118).
    \begin{align}
        &\frac{1}{2} \mathbf{x}^T P_i \mathbf{x}
        + \mathbf{q}_i^T \mathbf{x} + r_i \leq 0 \\
        \Leftrightarrow \ & \mathbf{x}^T P_i \mathbf{x}
        + 2(\mathbf{q}_i^T \mathbf{x} + r_i) \leq 0 \\
        \Leftrightarrow \ & \mathbf{x}^T P_i \mathbf{x}
        + 2(\mathbf{q}_i^T \mathbf{x} + r_i)
        + (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2
         \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2 \\
         \Leftrightarrow \ & \mathbf{x}^T P_i \mathbf{x}
         + (\mathbf{q}_i^T \mathbf{x} + r_i + \frac{1}{2})^2
          \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2
    \end{align}
    Since $P_i$ is positive semi-definite, $P_i = A_i^TA_i$, then
    \begin{align}
        \Leftrightarrow \ & \mathbf{x}^T P_i \mathbf{x}
        + (\mathbf{q}_i^T \mathbf{x} + r_i + \frac{1}{2})^2
         \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2 \\
         \Leftrightarrow \ & \parallel A_i \mathbf{x} \parallel^2
         + \parallel \mathbf{q}_i^T \mathbf{x} + r_i + \frac{1}{2}\parallel^2
          \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2 
    \end{align}
    Let
    \begin{align}
        A_i' &= \left(
            \begin{array}{ll}
                A \\
                \mathbf{q}^T
            \end{array}\right) \\
            \mathbf{b}_i &= \left(
                \begin{array}{ll}
                    \mathbf{0}_{n\times1} \\
                    r_i + \frac{1}{2}
                \end{array}\right)
    \end{align}
    From (123) and $\mathbf{x}^T P_i \mathbf{x} \geq 0$,
    we can derive that $\mathbf{q}_i^T \mathbf{x} + r_i \leq 0$,
    i.e., $\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2} \leq 0$.
    \par
    Then (128) can be formulated as
    \begin{align}
        &\parallel A_i'\mathbf{x} + \mathbf{b}_i \parallel^2
        \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2 \\
        \Leftrightarrow \ & \parallel A_i'\mathbf{x} + \mathbf{b}_i \parallel
        \leq -(\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})
    \end{align}
\end{proof}


\end{document}
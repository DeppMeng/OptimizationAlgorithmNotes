% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage{multirow, multicol}
\usepackage[ruled,vlined]{algorithm2e}
    
% \newtheorem{theorem}{Theorem}
% \newtheorem{theorem}{Theorem}[section]

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV18SubNumber{1452}  % Insert your submission number here

\title{Optimization Algorithm Notes} % Replace with your title

\titlerunning{ECCV-18 submission ID \ECCV18SubNumber}

\authorrunning{ECCV-18 submission ID \ECCV18SubNumber}

\author{Depu Meng}
\institute{Feb. $2019$}


\maketitle
\section{Introduction to Optimization Algorithms}
\subsection{Goal of the Course}
\begin{itemize}
    \item Understand foundations of optimization
    \item Learn to analyze widely used optimization algorithms
    \item Be familiar with implementation of optimization algorithms
\end{itemize}

\subsection{Topics Involved}
\begin{itemize}
    \item Unconstrained optimization
    \item Constrained optimization
    \item Convex optimization
    \item Sparse optimization
    \item Stochastic optimization
    \item Combinational optimization
    \item Global optimization
\end{itemize}

\subsection{Basic Concepts}
\noindent\textbf{Problem Definition}
Find the value of the decision variable s.t.
objective function is maximized/minimized
under certain conditions.
\begin{align}
    & \min f(x) \\
    & s.t. x \in \mathcal{S} \subset \mathbb{R}^n
\end{align}
Here, we call $\mathcal{S}$ \textit{feasible region}.
\par
We often denote constrained optimization Problem
 as 
\begin{align}
    & \min f(x) \\
    s.t.\quad & g_i(x) \geq 0, i = 1,..., n \\
    & b_i(x) = 0, i \in 1, ..., m
\end{align}

\begin{definition}{Global Optimality.}
    For global optimal value $x^* \in \mathcal{S}$,
    \begin{align}
        f(x^*) \leq f(x), \forall x \in \mathcal{S}
    \end{align}
\end{definition}

\begin{definition}{Local Optimality.}
    For local optimal value $x^* \in \mathcal{S}$, $\exists U(x^*)$,
    such that
    \begin{align}
        f(x^*) \leq f(x), \forall x \in \mathcal{S} \cap U(x^*)
    \end{align}
    \end{definition}

\begin{definition}{Feasible direction.}
    Let $x \in \mathcal{S}$, $d \in \mathbb{R}^n$
    is a non-zero vector. if $\exists \delta > 0$,
    such that
    \begin{align}
        x + \lambda d \in \mathcal{S}, \forall \lambda \in (0, \delta)
    \end{align}
    Then $d$ is a \textbf{feasible direction} at $x$.
    We denote $F(x, \mathcal{S})$ as the set of feasible directions
    at $x$.
\end{definition}

\begin{definition}{Descent direction.}
    $f(x): \mathbb{R}^n \rightarrow \mathbb{R}$, $x \in \mathbb{R}^n$,
    $d$ is a non-zero vector. If $\exists \delta > 0$, such that
    \begin{align}
        f(x + \lambda d) < f(x), \forall \lambda \in (0, \delta)
    \end{align}
    Then $d$ is a \textbf{descent direction} at $x$.
    We denote $D(x, f) = \{ d | \bigtriangledown f(x)^T d < 0 \}$
    as the set of descent direction at $x$.
\end{definition}

\subsection{Optimal Conditions}
\par\noindent
\textbf{Unconstrained Optimization}
\par
First-order necessary condition:
$f(x)$ is differentiable at $x$, 
\begin{align}
    \bigtriangledown f(x) = 0
\end{align}
\par
Second-order necessary condition:
$f(x)$ is second-order differentiable at $x$,
\begin{align}
    \bigtriangledown f(x) = 0 \\
    \bigtriangledown^2 f(x) \geq 0
\end{align}
\noindent
\textbf{Constrained Optimization}


\begin{theorem}{Fritz-John Condition}
    \par
    For constrained optimization problem
    \begin{align}
        & \min f(x) \\
        s.t.\quad & g_i(x) \geq 0, i = 1,..., n \\
        & h_i(x) = 0, i \in 1, ..., m
    \end{align}
    Denote $I(x) = \{i \in \{1,...,n\} | g_i(x) = 0\}$.
    For $x \in \mathcal{S}$, $f$ and $g_i, i \in I(x)$
    is differentiable at $x$, $h_j(x)$ is continuously
    differentiable at $x$.
    If $x$ is local optimal, then there exists non-trivial
    $\lambda_0, \lambda_i \geq 0, i \in I(x)$
    and $\mu_j$, such that
    \begin{align}
        \lambda_0 \bigtriangledown f(x) - \sum_{i \in I(x)} \lambda_i
        \bigtriangledown g_i(x) - \sum_{j=1}^m \mu_j
        \bigtriangledown h_j(x) = 0
    \end{align}
\end{theorem}

\begin{proof}
    (i) If $\{\bigtriangledown h_j (x)\}$ is linearly dependent,
    then there exists non-trivial $\mu_j$, such that
    \begin{align}
        \sum_{j=1}^m \bigtriangledown \mu_jh_j (x) = 0
    \end{align}
    Let $\lambda_0, \lambda_i, i \in I(x) = 0$, then (13) holds.
    \par
    (ii) If $\{\bigtriangledown h_j (x)\}$ is linearly independent,
    Denote
    \begin{align}
        F_g &= F(x, g) = \{ d | \bigtriangledown g_i(x)^Td > 0, i \in I(x)\} \\
        F_h &= F(x, h) = \{ d | \bigtriangledown h_j(x)^Td = 0, j = 1,...,m\}
    \end{align}
    If $x$ is a optimal value, then appearently 
    $F(x, \mathcal{S}) \cap D(x, f) = \varnothing$.
    Due to the independence of $\{\bigtriangledown h_j (x)\}$,
    we have {\color{red}$F_g \cap F_h \subset F(x, \mathcal{S})$},
    then 
    \begin{align}
        F_g \cap F_h \cap D(x, f) = \varnothing
    \end{align}
    that is
    \begin{align}
        \left\{
        \begin{array}{ll}
            \bigtriangledown f(x)^Td < 0 \\
            \bigtriangledown g_i(x)^Td > 0, i \in I(x) \\
            \bigtriangledown h_j (x)^Td  =0, j = 1,...,m
        \end{array}
        \right.
    \end{align}
    has no solution.
    Let 
    \begin{align}
        A &= \{ \bigtriangledown f(x)^T, -\bigtriangledown g_i(x)\}^T, i \in I(x) \\
        B &= \{ -\bigtriangledown h_j(x)\}, j = 1,...,m
    \end{align}
    Then (21) is equivalent to
    \begin{align}
        \left\{
        \begin{array}{ll}
            A^T d < 0 \\
            B^T d = 0
        \end{array}
        \right.
    \end{align}
    has no solution.
    \par
    \noindent
    Denote
    \begin{align}
        S_1 &= 
        \left\{
            \left(
            \begin{array}{ll}
                y_1 \\
                y_2
            \end{array}
            \right)
            | y_1 = A^Td, y_2 = B^Td, d \in \mathbb{R}^n
        \right\} \\
        S_2 &= 
        \left\{
            \left(
            \begin{array}{ll}
                y_1 \\
                y_2
            \end{array}
            \right)
            | y_1 < 0, y_2 = 0
        \right\} 
    \end{align}
    $S_1, S_2$ are non-trivial convex sets,
    and $S_1 \cap S_2 = \varnothing$.
    From \textit{Hyperplane Separation Theorem}:
    $\exists 
    \left(
    \begin{array}{ll}
        p_1 \\
        p_2
    \end{array}
    \right)$, such that
    \begin{align}
        p_1^TA^Td + p_2^TB^Td \geq p_1^Ty_1 + p_2^Ty_2,
        \forall d \in \mathbb{R}^n, 
        \forall\left(
        \begin{array}{ll}
            y_1 \\
            y_2
        \end{array}
        \right) \in CL(S_2)
    \end{align}
    Let $y_2 = 0, d = 0, y_1 < 0$, we have
    \begin{align}
        p_1 \geq 0
    \end{align}
    Let $\left(
        \begin{array}{ll}
            y_1 \\
            y_2
        \end{array}
        \right) = 
        \left(
        \begin{array}{ll}
            0 \\
            0
        \end{array}
        \right) 
         \in CL(S_2)$
    So that
    \begin{align}
        (p_1^TA^T + p_2^TB^T)d \geq 0 \\
        (Ap_1 + Bp_2)^Td \geq 0
    \end{align}
    Let $d = - (Ap_1 + Bp_2)$, we have
    \begin{align}
        Ap_1 + Bp_2 = 0
    \end{align}
    From above, we have
    \begin{align}
        \left\{
        \begin{array}{ll}
            Ap_1 + Bp_2 = 0 \\
            p_1 \geq 0
        \end{array}
        \right.
    \end{align}
    Let $p_1 = \{\lambda_0,..., \lambda_{I(x}\}$,
    $p_2 = \{\mu_1, ..., \mu_m\}$, i.e.,
    \begin{align}
        \left\{
            \begin{array}{ll}
            \lambda_0 \bigtriangledown f(x) - \sum_{i \in I(x)} \lambda_i
            \bigtriangledown g_i(x) - \sum_{j=1}^m \mu_j
            \bigtriangledown h_j(x) = 0 \\
            \lambda_i \geq 0
            \end{array}
            \right.
    \end{align}
\end{proof}

\begin{theorem}{Kuhn-Tucker Condition}
    \par
    For constrained optimization problem
    \begin{align}
        & \min f(x) \\
        s.t.\quad & g_i(x) \geq 0, i = 1,..., n \\
        & h_i(x) = 0, i \in 1, ..., m
    \end{align}
    Denote $I(x) = \{i \in \{1,...,n\} | g_i(x) = 0\}$.
    For $x \in \mathcal{S}$, $f$ and $g_i, i \in I(x)$
    is differentiable at $x$, $h_j(x)$ is continuously
    differentiable at $x$.
    $\{\bigtriangledown g_i(x), i \in I(x);\bigtriangledown h_j(x), 
    j = 1,...,m\}$ is linearly independent.
    If $x$ is local optimal, then $\exists \lambda_i \geq 0$
    and $\mu_j$, such that
    \begin{align}
        \bigtriangledown f(x) - \sum_{i \in I(x)} \lambda_i
        \bigtriangledown g_i(x) - \sum_{j=1}^m \mu_j
        \bigtriangledown h_j(x) = 0
    \end{align}
\end{theorem}
\subsection{Descent function}
\par
\begin{definition}{Descent function.}
    Denote solution set $\Omega \in X$, $\mathcal{A}$ is an
    algorithm on $X$, $\psi: X \rightarrow \mathbb{R}$.
    If
    \begin{align}
        \psi(y) < \psi(x),\quad \forall x \notin \Omega, y \in \mathcal{A}(x) \\
        \psi(y) \leq \psi(x),\quad \forall x \in \Omega, y \in \mathcal{A}(x)
    \end{align}
    Then $\psi$ is a \textbf{descent function} of $(\Omega, \mathcal{A})$.
\end{definition}
\subsection{Convergence of Algorithm}
\par
\begin{theorem}
    $\mathcal{A}$ is an algorithm on $X$,
    $\Omega$ is the solution set, $x^{(0)} \in X$.
    If $x^{(k)} \in \Omega$, then the iteration stops.
    Otherwise set $x^{(k+1)} = \mathcal{A}(x^{(k)}), k:=k+1$.
    If
    \begin{itemize}
        \item $\{x^{(k)}\}$ in a compact subset of $X$
        \item There exists a continuous function $\psi$,
         $\psi$ is a descent function of $(\Omega, \mathcal{A})$
        \item $\mathcal{A}$ is closed on $\Omega^C$
    \end{itemize}
    Then, any convergent subsequence of $\{x^{(k)}\}$ converges to 
    $x, x \in \Omega$.
\end{theorem}
\begin{proof}
    
\end{proof}

\subsection{Search Methods}
\noindent\textbf{Line Search}
\par
Generate $d^{(k)}$ from 
$x^{(k)}$, 
\begin{align}
    x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}
\end{align}.
search $\alpha_k$ in 1-D space.
\par
\noindent\textbf{Trust Region}
\par
Generate local model $Q_{k}(s)$ of $x^{(k)}$,
\begin{align}
    s^{(k)} = \mathop{\arg\min} Q_k(s) \\
    x^{(k+1)} = x^{(k)} + s^{(k)}
\end{align}

\clearpage
\section{Unconstrained Optimization}

\subsection{Gradient Based Methods}

\begin{align}
    \min_{x \in \mathbb{R}^n} f(x)
\end{align}
\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Solution set $\Omega$, cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, k:= 0$\;
     \While{$x^{(k)} \notin \Omega$}{
      $d^{(k)} = - H_k \bigtriangledown f(x^{(k)})$,
        ($H_k$ is a positive definite symmetrical matrix)\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$, $k:= k+1$
     }
     \caption{Example of gradient based algorithm}
\end{algorithm}
\subsection{Determine Search Direction}
\noindent\textbf{First-order gradient method}
\par
For unconstrained optimization problem 
\begin{align}
    \min_{x \in \mathbb{R}^n} f(x)
\end{align}
We have
\begin{align}
    f(x) = f(x^{(k)}) + \bigtriangledown f(x^{(k)})^T(x - x^{(k)})
    + O(\parallel x - x^{(k)} \parallel^2)
\end{align}
Set $d^{(k)} = - \bigtriangledown f(x^{(k)})$,
when $\alpha_k$ is sufficiently small,
\begin{align}
    f(x^{(k)} + \alpha_k d^{(k)}) < f(x^{(k)})
\end{align}

\noindent\textbf{Second-order gradient method -- Newton Direction}
\par
\begin{align}
    f(x) = &f(x^{(k)}) + \bigtriangledown f(x^{(k)})^T(x - x^{(k)}) \\
    &+ \frac{1}{2} (x - x^{(k)})^T \bigtriangledown^2 f(x^{(k)}) (x - x^{(k)})
    + O(\parallel x - x^{(k)} \parallel^3)
\end{align}
Set $d^{(k)} = -G_k^{-1} \bigtriangledown f(x^{(k)})$,
 where $G_k = \bigtriangledown^2 f(x^{(k)})$,
 i.e., Hesse matrix of $f$ at $x^{(k)}$.
\subsection{Determine Step Factor -- Line Search}

\begin{align}
    \min_{\alpha \geq 0} \varphi(\alpha) = f(x^{(k)} + \alpha d^{(k)})
\end{align}
\noindent\textbf{Exact Line Search}
\par
Solve Line Search problem in finite iterations.
\par
\noindent\textbf{Inexact Line Search}
\par
In some cases, the exact solution of Line Search
is not necessary, so we can use inexace line search
to improve algorithm efficiency.
\par
\noindent\textit{Goldstein Conditions}
\begin{align}
    \varphi(\alpha) &\leq \varphi(0) + \rho\alpha\varphi'(0) \\
    \varphi(\alpha) &\geq \varphi(0) + (1 - \rho)\alpha\varphi'(0)
\end{align}
where$\rho \in (\frac{1}{2}, 1)$ is a fixed parameter.
\par
However, the downside of Goldstein Conditions
is that the optimal value might not lie in
the valid area.
\par\noindent\textit{Wolfe-Powell Conditions}
\begin{align}
    \varphi(\alpha) &\leq \varphi(0) + \rho\alpha\varphi'(0) \\
    \varphi'(\alpha) &\geq \sigma\varphi'(0)
\end{align}
where $\sigma \in (\rho, 1)$.

\subsection{Global Convergence}
\begin{theorem}
    Assume $\bigtriangledown f(x)$ exists and uniformly continuous
    on level set $L(x^{(0)}) = \{ x | f(x) \leq f(x^{(0)})\}$.
    Denote $\theta^{(k)}$ as the angle between $d^{(k)}$
    and $-\bigtriangledown f(x^{(k)})$.
    \begin{align}
        \theta^{(k)} \leq \frac{\pi}{2} - \mu
    \end{align}
    If step factor is determined by following methods
    \begin{itemize}
        \item Exace Line Search
        \item Goldstein Conditions
        \item Wolfe-Powell Conditions
    \end{itemize}
    Then, there exists $k$, such that $\bigtriangledown f(x^{(k)}) = 0$,
    or $f(x^{(k)}) \rightarrow 0$ or $f(x^{(k)}) \rightarrow - \infty$.
\end{theorem}
\begin{proof}
    
\end{proof}

\subsection{Steepest Descent Method}
Steepest Descent Method is a Line Search Method.
\begin{align}
    x^{(k+1)} = x^{(k)} - \alpha_k \bigtriangledown f(x^{(k)})
\end{align}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Termination error $\epsilon$, cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, k:= 0$\;
     \While{$\parallel g^{(k)} \parallel \geq \epsilon$}{
        $d^{(k)} = - g^{(k)}$\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$, $k:= k+1$\;
        Compute $g^{(k)} = \bigtriangledown f(x^{(k)})$
     }
     \caption{Steepest Descent Algorithm}
\end{algorithm}
Steepest Descent Method has linear convergence rate generally.

\subsection{Newton Method}

Newton Method is also a Line Search Method.
\begin{align}
    f(x^{(k)} + s) \approx q^{(k)}(s) f(x^{(k)}) + g^{(k)^T} s
     + \frac{1}{2} s^T G_k s
\end{align}
where $g^{(k)} = \bigtriangledown f(x^{(k)})$,
$G_k = \bigtriangledown^2 f(x^{(k)})$.
To minimize $q^{(k)}(s)$, we have
\begin{align}
    s = G_k^{-1} g^{(k)}
\end{align}
Notice that $G_k^{-1} g^{(k)}$ is the Newton Direction.
\par
\vspace{.3em}
\noindent\textit{Analysis on quadratic function}
\par
For positive definite quadratic function
\begin{align}
    f(x) = \frac{1}{2} x^TGx - c^Tx
\end{align}
In this case, $\bigtriangledown^2 f(x) = G$.
Let $H_0 = G^{-1}$, then we have
\begin{align}
    d^{(0)} &= H_0 \bigtriangledown f(x^{(0)}) \\
    &= G^{-1} (Gx^{(0)} - c) \\
    &= x^{(0)} - G^{-1}c \\
    &= x^{(0)} - x^*
\end{align}
So that Newton Method can reach global optimal
in $1$ iteration for quadratic functions.
\par
For general non-linear functions, if we follow
\begin{align}
    x^{(k+1)} = x^{(k)} - G_k^{-1} g^{(k)}
\end{align}
we called it Newton Method.
\par
\vspace{.3em}
\noindent\textit{Convergence Rate of Newton Method}
\begin{theorem}
    $f \in \mathcal{C}^2$, $x^{(k)}$ is sufficiently closed
    to optimal point $x^*$, where $\bigtriangledown f(x^*) = 0$.
    If $\bigtriangledown^2 f(x^*)$ is positive definite, 
    Hesse matrix of $f$ satisfies Lipschitz Condition, i.e.,
    $\exists \beta >0$, such that for all $(i, j)$,
    \begin{align}
        | G_{ij}(x) - G_{ij}(y)| \leq \beta \parallel x - y \parallel
    \end{align}
    Then $\{x^{(k)}\} \rightarrow x^*$, and have quadratic
    convergence rate.
\end{theorem}
\begin{proof}
    Denote $g(x) = \bigtriangledown f(x)$, then we have
    \begin{align}
        g(x - h) = g(x) - G(x)h + O(\parallel h \parallel^2)
    \end{align}
    Let $x = x^{(k)}$, $h = h^{(k)} = x^{(k)} - x^*$, then
    \begin{align}
        g(x^*) = g(x^{(k)}) - G(x^{(k)})(h^{(k)})
        + O(\parallel h^{(k)} \parallel^2) = 0
    \end{align}
    From Lipschitz Condition, we can easily get 
    $G(x^{(k)})^{-1}$ is finite. Then we left multiply
    $G(x^{(k)})^{-1}$ to Equation (66)
    \begin{align}
        0 &= G(x^{(k)})^{-1} g(x^{(k)}) - h^{(k)}
         + O(\parallel h^{(k)} \parallel^2) \\
        &= x^* - x^{(k)} + G(x^{(k)})^{-1} g(x^{(k)})
         + O(\parallel h^{(k)} \parallel^2) \\
         &= x^* - x^{(k+1)}
         + O(\parallel h^{(k)} \parallel^2) \\
         &= - h^{(k+1)} + O(\parallel h^{(k)} \parallel^2)
    \end{align}
    i.e.,
    \begin{align}
        \parallel h^{(k+1)} \parallel = O(\parallel h^{(k)} \parallel^2)
    \end{align}
\end{proof}

\subsection{Quasi-Newton Methods}
Newton Method has a fast convergence rate.
However, Newton Method requires second-order derivative,
if Hesse matrix is not positive definite, Newton Method
might not work well.
\par
In order to overcome the above difficulties,
Quasi-Newton Method is introduced.
Its basic idea is that:
Using second-order derivative free matrix $H_k$
to approximate $G(x^{(k)})^{-1}$.
Denote $s^{(k)} = x^{(k+1)} - x^{(k)}$,
$y^{(k)} = \bigtriangledown f(x^{(k+1)}) - \bigtriangledown f(x^{(k)})$,
then we have
\begin{align}
    \bigtriangledown^2 f(x^{(k)}) s^{(k)} \approx y^{(k)}
\end{align}
or
\begin{align}
    \bigtriangledown^2 f(x^{(k)})^{-1} y^{(k)} \approx s^{(k)}
\end{align}
So we need to construct $H_{k+1}$ such that
\begin{align}
    H_{k+1} y^{(k)} \approx s^{(k)}
\end{align}
or
\begin{align}
    y^{(k)} \approx B_{k+1} s^{(k)}
\end{align}
we called (74), (75) \textit{Quasi-Newton Conditions}
or \textit{Secant Conditions}.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n, H_0 = I, k:= 0$\;
     \While{some conditions}{
        $d^{(k)} = - H_kg^{(k)}$\;
        solve $\min_{\alpha_k \geq 0} f(x^{(k)} + \alpha_k d^{(k)})$\;
        $x^{(k+1)} = x^{(k)} + \alpha_kd^{(k)}$\;
        generate $H_{k+1}$, $k:= k+1$
     }
     \caption{Quasi-Newton Algorithm}
\end{algorithm}
\vspace{.3em}
\noindent\textbf{How to generate $H_k$}
\par
$H_k$ is the approximation matrix in $k$th iteration, we want to
generate $H_{k+1}$ from $H_k$
\par
\vspace{.3em}
\noindent\textit{Symmetric Rank 1}
\par
Assume
\begin{align}
    H_{k+1} = H_k + a \mathbf{u}\mathbf{u}^T,
     \quad a \in \mathbb{R}, \mathbf{u} \in \mathbb{R}^n
\end{align}
From the Quasi-Newton Conditions, we have
\begin{align}
    H_{k+1} \mathbf{y}^{(k)} &= \mathbf{s}^{(k)} \\
    H_k \mathbf{y}^{(k)} + a \mathbf{u}\mathbf{u}^T \mathbf{y}^{(k)}
    &= \mathbf{s}^{(k)} \\
    H_k \mathbf{y}^{(k)} + a \mathbf{u}^T\mathbf{y}^{(k)} \mathbf{u} 
    &= \mathbf{s}^{(k)}
\end{align}
Let $\mathbf{u} = \mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)}$, 
$a = \frac{1}{\mathbf{u}^T \mathbf{y}}$,
clearly this is a solution of the equation.
Here we have
\begin{align}
    H_{k+1} = \frac{(\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})
    (\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})^T}
    {(\mathbf{s}^{(k)} - H_k \mathbf{y}^{(k)})^T \mathbf{y}^{(k)}}
\end{align}
(79) is \textit{Symmetric Rank 1 Update}.
The problem of Symmetric Rank 1 Update is that
the positive-definite property of $H_k$
can not be preserved.

\vspace{.3em}
\par
\noindent\textit{Symmetric Rank 2 Update}
\par
Assume
\begin{align}
    H_{k+1} = H_k + a \mathbf{u}\mathbf{u}^T +
     b \mathbf{v}\mathbf{v}^T,
     \quad a, b \in \mathbb{R}, \mathbf{u}, \mathbf{v} \in \mathbb{R}^n
\end{align}
such that Quasi-Newton Conditions stand.
We can find a solution of $a, b, \mathbf{u}, \mathbf{v}$ that is
\begin{align}
    \left\{
    \begin{array}{ll}
        \mathbf{u} = \mathbf{s}^{(k)}, 
        \quad a \mathbf{u}^T \mathbf{y} = 1 \\
        \mathbf{v} = H_k \mathbf{y}^{(k)}, 
        \quad b \mathbf{v}^T \mathbf{y} = -1
    \end{array}
    \right.
\end{align}
So that we have
\begin{align}
    H_{k+1} = H_k + \frac{\mathbf{s}^{(k)}\mathbf{s}^{(k)T}}
    {\mathbf{s}^{(k)T} \mathbf{y}^{(k)}} - 
    \frac{H_k\mathbf{y}^{(k)} \mathbf{y}^{(k)T} H_k}
    {\mathbf{y}^{(k)T}H_k\mathbf{y}^{(k)}}
\end{align}
We called (83) the DFP (Davidon-Fletcher-Powell) update.
\par
From Quasi-Newton Condition (75), we can get the
BFGS (Broyden-Fletcher-Goldfarb-Shanno) update
\begin{align}
    B_{k+1}^{(BFGS)} = B_k +
    \frac{\mathbf{y}^{(k)}\mathbf{y}^{(k)T}}
    {\mathbf{y}^{(k)T} \mathbf{s}^{(k)}}
    - \frac{B_k \mathbf{s}^{(k)}\mathbf{s}^{(k)T} B_k}
    {\mathbf{s}^{(k)T} B_k \mathbf{s}^{(k)}}
\end{align}
\vspace{.3em}
\par\noindent\textit{Inverse of SR1 update}
\begin{theorem}[Sherman-Morrison]
    $A \in \mathbb{R}^n \times \mathbb{R}^n$ is a 
    non-singular matrix, $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$.
    If $1 + \mathbf{v}^TA^{-1}\mathbf{u} \neq 0$,
    then SR1 update of $A$ is non-singular, and its inverse
    can be represented as
    \begin{align}
        (A + a\mathbf{u}\mathbf{v}^T)^{-1} = 
        A^{-1} - \frac{A^{-1}\mathbf{u}\mathbf{v}^TA^{-1}}
        {1 + \mathbf{v}^TA^{-1}\mathbf{u}}
    \end{align}
\end{theorem}

\subsection{Conjugate Gradient Method}
\begin{definition}{Conjugate Direction.}
    $G$ is a $n \times n$ positive definite matrix, 
    \par\noindent for non-zero vector set
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\} \in \mathbb{R}^n$,
    if $\mathbf{d}^{(i)T} G \mathbf{d}^{(j)} = 0, (i \neq j)$,
    then we called $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$
    is \emph{G-Conjugate}.
    \label{def:Conjugate}
\end{definition}

\begin{lemma}
    For non-zero conjugate vector set 
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\} \in \mathbb{R}^n$,
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ are linearly independent.
\end{lemma}
\begin{proof}
    From Definition \ref{def:Conjugate}, we have
    \begin{align}
        \mathbf{d}^{(i)T} G \mathbf{d}^{(j)} = 0, \forall i, j, i \neq j
    \end{align}
    if $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ is linearly dependent,
    there exists 
    \begin{align}
        \mathbf{d}^{(t)} = \sum_{j=0}^k c_j \mathbf{d}^{(j)}
    \end{align} 
    then
    \begin{align}
        \mathbf{d}^{(t)T} G \mathbf{d}^{(i)} = 
        \sum_{j=0}^k c_j \mathbf{d}^{(j)} G \mathbf{d}^{(i)}
        = c_i \mathbf{d}^{(i)} G \mathbf{d}^{(i)} \neq 0
    \end{align}
    so that $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$
    are linearly independent.
    \qed
    \label{lemma:Conjugate}
\end{proof}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Cost function $f$}
    % \KwResult{Write here the result }
     $x^{(0)} \in \mathbb{R}^n$, positive definite matrix $G$, $k:= 0$\;
     Construct $\mathbf{d}^{(0)}$ such that
     $\mathbf{g}^{(0)T}\mathbf{d}^{(0)} < 0$\;
     \While{some conditions}{
        solve $\min_{\alpha_k \geq 0} f(\mathbf{x}^{(k)} + \alpha_k \mathbf{d}^{(k)})$\;
        $\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k\mathbf{d}^{(k)}$\;
        Construct $\mathbf{d}^{(k+1)}$ such that 
        $\mathbf{d}^{(k+1)}G\mathbf{d}^{(j)} = 0, j = 0,..., k$.\;
        $k:=k+1$
     }
     \caption{Conjuagte Gradient Algorithm}
\end{algorithm}

\begin{theorem}[Conjugate Gradient]
    For strictly convex quadratic function 
    \par\noindent
    $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T G \mathbf{x} + \mathbf{c}^T\mathbf{x}$,
    apply conjugate gradient method combined with exact line search,
    then $\mathbf{x}^{(k+1)}$ is the global minima in manifold
    \begin{align}
        \mathcal{V} =  \{ \mathbf{x} | \mathbf{x} = \mathbf{x}^{(0)} + 
        \sum_{j=0}^k\beta_j\mathbf{d}^{(j)}, \forall \beta_j \in \mathbb{R} \}
    \end{align}
    \label{th:Conjugate}
\end{theorem}
\begin{proof}
    Firstly, from Lemma \ref{lemma:Conjugate}, we have 
    $\{ \mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$ are linearly independent.
    So we only need to prove that 
    for all $k < n$
    \begin{align}
        \mathbf{g}^{(k+1)T}\mathbf{d}^{(j)} = 0, j = 0,..., k
    \end{align}
    i.e., $\mathbf{g}^{(k+1)}$ is orthogonal with subspace
    $span\{\mathbf{d}^{(0)}, ...,\mathbf{d}^{(k)}\}$.
    \par
    Due to the exact line search, $\forall j$
    \begin{align}
        \mathbf{g}^{(j+1)T}\mathbf{d}^{(j)} = 0
    \end{align}
    especially $\mathbf{g}^{(k+1)T}\mathbf{d}^{(k)} = 0$.
    \par
    Notice that 
    \begin{align}
        \mathbf{g}^{(k+1)} - \mathbf{g}^{(k)}
        = G (\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)})
        = \alpha_k G \mathbf{d}^{(k)}
    \end{align}
    so that we have $\forall j \leq k$
    \begin{align}
        \mathbf{g}^{(k+1)T}\mathbf{d}^{(j)}
        &= (\sum_{m=j+1}^k (\mathbf{g}^{(m+1)T} - \mathbf{g}^{(m)T})
        + \mathbf{g}^{(j+1)T} )\mathbf{d}^{(j)} \\
        &= \sum_{m=j+1} \alpha_m \mathbf{d}^{(m)T}G \mathbf{d}^{(j)}
        + \mathbf{g}^{(j+1)T} \mathbf{d}^{(j)} \\
        &= 0
    \end{align}
    \qed
\end{proof}
\begin{lemma}
    For strictly convex quadratic function 
    $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T G \mathbf{x} + \mathbf{c}^T\mathbf{x}$,
    apply conjugate gradient method combined with exact line search,
    $\mathbf{g}(\mathbf{x}) = \bigtriangledown f(\mathbf{x}) = G\mathbf{x} + \mathbf{c}$,
    we have
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)} = 0, \forall j = 0,..., k-1
    \end{align}
    \label{lemma:Conjugate}
\end{lemma}
\begin{proof}
    From Theorem \ref{th:Conjugate}, we have
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)}
        = \mathbf{g}^{(k)T} (-\mathbf{d}^{(j)} 
        + \sum_{i=0}^{j-1}\beta_i^{(j)} \mathbf{d}^{(i)}) = 0
    \end{align}
    \qed
\end{proof}
\noindent\textbf{Quadratic function case}
\par
For $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T G \mathbf{x} + \mathbf{c}^T x$,
$G$ is a $n \times n$ positive definite matrix.
\begin{align}
    \mathbf{g}(\mathbf{x}) = G \mathbf{x} + \mathbf{c}
\end{align}
Set $\mathbf{d}^{(0)} = -\mathbf{g}^{(0)}$,
exact line search for $\alpha_0$ such that 
$\mathbf{g}^{(1)T}\mathbf{d}^{(0)} = 0$.
Assume $\mathbf{d}^{(1)} = -\mathbf{g}^{(1)} + \beta_0^{(1)}\mathbf{d}^{(0)}$,
select $\beta_0^{(1)}$ such that
$ \mathbf{d}^{(1)} G \mathbf{d}^{(0)} = 0$
\begin{align}
    \beta_0^{(1)} = \frac{\mathbf{g}^{(1)T}\mathbf{g}^{(1)}}
    {\mathbf{g}^{(0)T}\mathbf{g}^{(0)}}
\end{align}
\begin{proof}
    From (92), we have
    \begin{align}
        &\mathbf{d}^{(1)T} G \mathbf{d}^{(0)} = 0 \\
        \Leftrightarrow \quad&\mathbf{d}^{(1)T}
        (\mathbf{g}^{(1)} - \mathbf{g}^{(0)}) = 0 \\
        \Leftrightarrow  \quad&(\mathbf{g}^{(1)} + 
        \beta_0^{(1)}\mathbf{g}^{(0)})^T
        (\mathbf{g}^{(1)} - \mathbf{g}^{(0)}) = 0 \\
        \Leftrightarrow \quad&\mathbf{g}^{(1)T}\mathbf{g}^{(1)}
        - \beta_0^{(1)}\mathbf{g}^{(0)T}\mathbf{g}^{(0)} = 0 \\
        \Leftrightarrow \quad&\beta_0^{(1)} = 
        \frac{\mathbf{g}^{(1)T}\mathbf{g}^{(1)}}
        {\mathbf{g}^{(0)T}\mathbf{g}^{(0)}}
    \end{align}
    \qed
\end{proof}
    Generally, we can select $\beta_j^{(k)}$ such that
    $\mathbf{d}^{(k)T} G \mathbf{d}^{(j)} = 0, j=0, 1,..., k-1$
    that is
    \begin{align}
        \mathbf{d}^{(k)T} G \mathbf{d}^{(j)} &= 0 \\
        (-\mathbf{g}^{(k)T} + \sum_{i=0}^{k-1} \beta_i^{(k)} \mathbf{d}^{(i)T})
        G \mathbf{d}^{(j)} &= 0 \\
        -\mathbf{g}^{(k)T} G \mathbf{d}^{(j)}
        + \beta_j^{(k)}\mathbf{d}^{(j)T} G \mathbf{d}^{(j)} &= 0
    \end{align}
    so we have
    \begin{align}
        \beta_j^{(k)} = \frac{\mathbf{g}^{(k)T} G \mathbf{d}^{(j)}}
        {\mathbf{d}^{(j)T} G \mathbf{d}^{(j)}}
        = \frac{\mathbf{g}^{(k)T} (\mathbf{g}^{(j+1)} - \mathbf{g}^{(j)})}
        {\mathbf{d}^{(j)T}(\mathbf{g}^{(j+1)} - \mathbf{g}^{(j)})}
    \end{align}
    From Lemma \ref{lemma:Conjugate}, we have 
    \begin{align}
        \mathbf{g}^{(k)T}\mathbf{g}^{(j)} = 0, \forall j = 0,..., k-1
    \end{align}
    So
    \begin{align}
        \beta_j^{(k)} &= 0, j = 0,...,k-2 \\
        \beta_{k-1}^{(k)} &= \frac{\mathbf{g}^{(k)T} (\mathbf{g}^{(k)}}{\mathbf{g}^{(k-1)T} (\mathbf{g}^{(k-1)}}
    \end{align}

\subsection{Trust Region Method}

\clearpage
\section{Constrained Optimization}
\subsection{Quadratic Programming}
\subsection{Non-linear Constrained Optimization}
\end{document}
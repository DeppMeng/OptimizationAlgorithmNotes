\chapter{Convex Optimization}
\vspace{1em}
\section{Convex set}
\subsection{Affine set}
\begin{definition}[Affine set]
    A set $\mathcal{C} \subset \mathbb{R}^n$ is affine
    if $\mathbf{x}_1, \mathbf{x}_2 \in \mathcal{C}$
    and $\theta \in \mathbb{R}$, we have
    \begin{align}
        \theta \mathbf{x}_1 + 
        (1 - \theta)\mathbf{x}_2
        \in \mathcal{C}
    \end{align}
\end{definition}

\begin{definition}[Affine hull]
    The set of all affine combinations of points
    in some set $\mathcal{C} \subset \mathbb{R}^n$
    is called the \emph{affine hull} of $\mathcal{C}$,
    denoted $\mathbf{aff}\mathcal{C}$:
    \begin{align}
        \mathbf{aff}\mathcal{C} = \{
            \sum_{i=1}^k \theta_i \mathbf{x}_i \ | \ 
            \mathbf{x}_1,...,\mathbf{x}_k \in \mathcal{C},
            \theta_1 + ... + \theta_k = 1\}
    \end{align}
\end{definition}
\begin{remark}
    The affine hull is the smallest affine set
    that contains $\mathcal{C}$.
\end{remark}
\begin{proof}
    For any affine set $\mathcal{A}$ contains $\mathcal{C}$,
    we have
    \begin{align}
        \sum_{i=1}^k \theta_i \mathbf{x}_i \in \mathcal{A},
        \forall \mathbf{x}_1,...,\mathbf{x}_k \in \mathcal{C},
            \theta_1 + ... + \theta_k = 1
    \end{align}
    i.e., $\mathbf{aff}\mathcal{C} \subset \mathcal{A}$.
\end{proof}

\subsection{Convex set}
\begin{definition}[Convex set]
    A set $\mathcal{C} \subset \mathbb{R}^n$ is convex
    if $\mathbf{x}_1, \mathbf{x}_2 \in \mathcal{C}$
    and $0 \leq \theta \leq 1$, we have
    \begin{align}
        \theta \mathbf{x}_1 + 
        (1 - \theta)\mathbf{x}_2
        \in \mathcal{C}
    \end{align}
\end{definition}
\begin{definition}[Convex hull]
    The set of all convex combinations of points
    in some set $\mathcal{C} \subset \mathbb{R}^n$
    is called the \emph{convex hull} of $\mathcal{C}$,
    denoted $\mathbf{conv}\mathcal{C}$:
    \begin{align}
        \mathbf{conv}\mathcal{C} = \{
            \sum_{i=1}^k \theta_i \mathbf{x}_i \ | \ 
            \mathbf{x}_1,...,\mathbf{x}_k \in \mathcal{C},
            \theta_i \geq 0,
            \theta_1 + ... + \theta_k = 1\}
    \end{align}
\end{definition}
\begin{remark}
    The convex hull is the smallest convex set
    that contains $\mathcal{C}$.
\end{remark}

\subsection{Cone}
\begin{definition}[Cone]
    A set $\mathcal{C}$ is called a \emph{cone},
    if $\forall \mathbf{x} \in \mathcal{C}$ and
    $\theta \geq 0$ we have $\theta \mathbf{x} \in \mathcal{C}$.
    A set $\mathcal{C}$ is called a \emph{convex cone}
    if it is convex and a cone, i.e.,
    $\forall \mathbf{x}_1, \mathbf{x})_2 \in \mathcal{C}$
    and $\theta_1, \theta_2 \geq 0$, we have
    \begin{align}
        \theta_1 \mathbf{x}_1 + \theta_2 \mathbf{x}_2
        \in \mathcal{C}
    \end{align}
\end{definition}

\begin{definition}[Conic hull]
    The conic hull of set $\mathcal{C}$ is the
    set of all conic combinations of points in
    $\mathcal{C}$, i.e.,
    \begin{align}
        \{ \sum_{i=1}^k \theta_i \mathbf{x}_i \ | \
        \mathbf{x}_i \in \mathcal{C}, \theta_i \geq 0,
        i = 1,...,k \}
    \end{align}
\end{definition}

\subsection{Proper cones and generalized inequalities}



\section{Convex function}
\begin{definition}[Convex function]
    A function $f:\mathbb{R}^n \rightarrow \mathbb{R}$
    is \emph{convex} if $\mathbf{dom}f$ is a convex
    set and if $\forall x, y \in \mathbf{dom}f$ and
    $\theta$ with $0 \leq \theta\leq 1$, we have
    \begin{align}
        f(\theta x_1 + (1 - \theta) x_2) \leq
        \theta f(x_1) + (1 - \theta) f(x_2)
    \end{align}
\end{definition}

\subsection{First order condition}
Suppose $f$ is differentiable
\begin{theorem}
    Function $f$ is convex if and only if $\mathbf{dom}f$
    is a convex set and for $\forall x, y \in \mathbf{dom}f$,
    the following holds:
    \begin{align}
        f(y) \geq f(x) + \bigtriangledown f(x)^T (y - x)
    \end{align}
    \label{th:conv2}
\end{theorem}

\begin{remark}
    If $\bigtriangledown f(x^*) = 0$, then for
    $\forall y \in \mathbf{dom}f$, 
    $f(y) \geq f(x^*)$, i.e., $x^*$
    is the global minimizer of $f$.
\end{remark}

\subsection{Second order condition}
Suppose $f$ is twice differentiable
\begin{theorem}
    Function $f$ is convex if and only if $\mathbf{dom}f$
    is a convex set and for $\forall x \mathbf{dom}f$,
    the following holds:
    \begin{align}
        \bigtriangledown^2 f(x) \succeq 0 
    \end{align}
\end{theorem}
\begin{remark}
    If $\bigtriangledown^2 f(x) \succ 0$ for
    $\forall x \mathbf{dom}f$, then $f$ is
    \emph{strictly convex}.
\end{remark}

\subsection{Properties of Convex functions}
\subsubsection{Jensen's Inequality}
\begin{theorem}[Jensen's Inequality]
    If $f$ is convex, $x_1,..., x_k \in \mathbf{dom}f$,
    and $\theta_1,...,\theta_k \geq 0$ with
    $\theta_1 + ... + \theta_k = 1$, then
    \begin{align}
        f(\theta_1 x_1 + ... + \theta_k x_k)
        \leq \theta_1 f(x_1) + ... + \theta_k f(x_k)
    \end{align}
\end{theorem}

\subsubsection{Operations that preserve convexity}
\paragraph{Nonnegative weighted sums}
If $f_1,..., f_m$ are covex and $w_1,...,w_m \geq 0$,
then
\begin{align}
    f = w_1 f_1 + ... + w_m f_m
\end{align}
is convex.
\par
If $f(x, y)$ is convex w.r.t $x$ for each
$y \in \mathcal{A}$, and $w(y) \geq 0$ for each
$y \in \mathcal{A}$, then the function
\begin{align}
    g(x) = \int_{\mathcal{A}} w(y) f(x, y) dy
\end{align}
is convex w.r.t $x$.

\section{Convex optimization}
A \emph{convex optimization problem}
is one of the form
\begin{align}
    \begin{array}{lll}
        \min \ &f_0 (\mathbf{x}) \\
        s.t. \ &f_i (\mathbf{x}) \leq 0, &i = 1,..., m \\
        &a_j^T \mathbf{x} = b_j, &j = 1, ..., p
    \end{array}
    \label{pro:conv1}
\end{align} 
where $f_0,..., f_m$ are convex functions.
\begin{remark}
    The equality constraint is linear if the problem
    is convex.
\end{remark}
\begin{proof}
    For equality constraint
    \begin{align}
        \mathbf{c}(\mathbf{x}) = 0
    \end{align}
    we can rewrite it into
    \begin{align}
        \mathbf{c}(\mathbf{x}) \leq 0 \\
        -\mathbf{c}(\mathbf{x}) \leq 0
    \end{align}
    Due to the convexity of the problem,
    both $\mathbf{c}(\mathbf{x})$ and
    $-\mathbf{c}(\mathbf{x})$ are convex.
    i.e., $\mathbf{c}(\mathbf{x})$ is linear.
\end{proof}

\subsection{Optimal condition}

\begin{theorem}[Optimal condition]
Suppose (\ref{pro:conv1}) is differentiable.
Let $S$ denote the feasible set, then $\mathbf{x}^*$
is optimal if and only if $\mathbf{x}^* \in S$ and
\begin{align}
    \bigtriangledown f_0(\mathbf{x})^T (\mathbf{y}
    - \mathbf{x}) \geq 0, \forall y \in S
    \label{equ:conv1}
\end{align}
\label{th:conv1}
\end{theorem}
\begin{proof}
    If $\mathbf{x}^*$ is optimal, then
    we can easily derive (\ref{equ:conv1}).
    \par
    If (\ref{equ:conv1}) stands, then from
    Theorem \ref{th:conv2},
    \begin{align}
        f(\mathbf{y}) - f(\mathbf{x})
        \geq \bigtriangledown f_0(\mathbf{x})^T (\mathbf{y}
        - \mathbf{x}) \geq 0, \forall y \in S
    \end{align}
\end{proof}

\begin{lemma}
For convex problem with equality constraints only,
i.e.,
\begin{align}
    \begin{array}{ll}
        \min \ &f_0 (\mathbf{x}) \\
        s.t. \ &A (\mathbf{x}) = \mathbf{b} 
    \end{array}
\end{align}
the optimal condition can be expressed as
\begin{align}
    \bigtriangledown f_0(\mathbf{x})^T \mathbf{u}
    \geq 0, \forall \mathbf{u} \in \mathcal{N}(A)
\end{align}
in other words,
\begin{align}
    \bigtriangledown f_0(\mathbf{x}) \perp
    \mathcal{N}(A)
\end{align}
\end{lemma}

\begin{proof}
    From Theorem \ref{th:conv1}, we have
    $\mathbf{x}^*$ is optimal if and only if
    $A\mathbf{x} = \mathbf{b}$, for $\forall \mathbf{y}$
    such that $A\mathbf{y} = \mathbf{b}$,
    \begin{align}
        \bigtriangledown f_0(\mathbf{x})^T (\mathbf{y}
    - \mathbf{x}) \geq 0
    \end{align}
    i.e., $A(\mathbf{y} - \mathbf{x}) = 0$.
    Let $\mathbf{u} = \mathbf{y} - \mathbf{x}$,
    then
    \begin{align}
        \bigtriangledown f_0(\mathbf{x})^T \mathbf{u}
        \geq 0, \forall \mathbf{u} \in \mathcal{N}(A)
    \end{align}
    further, if $\mathbf{u} \in \mathcal{N}(A)$,
    then, $-\mathbf{u} \in \mathcal{N}(A)$, so we have
    \begin{align}
        \bigtriangledown f_0(\mathbf{x})^T \mathbf{u}
        = 0, \forall \mathbf{u} \in \mathcal{N}(A)
    \end{align}
    i.e.,
    \begin{align}
        \bigtriangledown f_0(\mathbf{x}) \perp
        \mathcal{N}(A)
    \end{align}
\end{proof}

\subsection{Linear optimization}

\subsection{Quadratic optimization}

\subsubsection{Quadratically constrained quadratic program}
\begin{align}
    \begin{array}{lll}
        \min \ &\frac{1}{2} \mathbf{x}^T P_0 \mathbf{x}
        + \mathbf{q}_0^T \mathbf{x} + r_0 \\
        s.t. \ &\frac{1}{2} \mathbf{x}^T P_i \mathbf{x}
        + \mathbf{q}_i^T \mathbf{x} + r_i \leq 0, \quad i = 1,...,m \\
        & A \mathbf{x} = \mathbf{b}
    \end{array}
\end{align}


\par
\subsubsection{Second-order cone program}
\begin{align}
    \begin{array}{lll}
        \min \ & \mathbf{f}^T \mathbf{x} \\
        s.t. \ & \parallel A_i \mathbf{x} + \mathbf{b}_i \parallel
        \leq \mathbf{c}_i^T\mathbf{x} + \mathbf{d}_i, \quad i = 1,...,m \\
        & F\mathbf{x} = \mathbf{g}
    \end{array}
\end{align}

\begin{lemma}
    Any QCQP problem can be formulated as a SOCP problem.
\end{lemma}
\begin{proof}
    The QCQP problem is equivalent to
    \begin{align}
        \min \ & - r_0 \\
        s.t. \ &\frac{1}{2} \mathbf{x}^T P_i \mathbf{x}
        + \mathbf{q}_i^T \mathbf{x} + r_i \leq 0, \quad i = 0,...,m \\
        & A \mathbf{x} = \mathbf{b}
    \end{align}
    Then we need to prove that (121) can be formulated as (118).
    \begin{align}
        &\frac{1}{2} \mathbf{x}^T P_i \mathbf{x}
        + \mathbf{q}_i^T \mathbf{x} + r_i \leq 0 \\
        \Leftrightarrow \ & \mathbf{x}^T P_i \mathbf{x}
        + 2(\mathbf{q}_i^T \mathbf{x} + r_i) \leq 0 \\
        \Leftrightarrow \ & \mathbf{x}^T P_i \mathbf{x}
        + 2(\mathbf{q}_i^T \mathbf{x} + r_i)
        + (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2
         \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2 \\
         \Leftrightarrow \ & \mathbf{x}^T P_i \mathbf{x}
         + (\mathbf{q}_i^T \mathbf{x} + r_i + \frac{1}{2})^2
          \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2
    \end{align}
    Since $P_i$ is positive semi-definite, $P_i = A_i^TA_i$, then
    \begin{align}
        \Leftrightarrow \ & \mathbf{x}^T P_i \mathbf{x}
        + (\mathbf{q}_i^T \mathbf{x} + r_i + \frac{1}{2})^2
         \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2 \\
         \Leftrightarrow \ & \parallel A_i \mathbf{x} \parallel^2
         + \parallel \mathbf{q}_i^T \mathbf{x} + r_i + \frac{1}{2}\parallel^2
          \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2 
    \end{align}
    Let
    \begin{align}
        A_i' &= \left(
            \begin{array}{ll}
                A \\
                \mathbf{q}^T
            \end{array}\right) \\
            \mathbf{b}_i &= \left(
                \begin{array}{ll}
                    \mathbf{0}_{n\times1} \\
                    r_i + \frac{1}{2}
                \end{array}\right)
    \end{align}
    From (123) and $\mathbf{x}^T P_i \mathbf{x} \geq 0$,
    we can derive that $\mathbf{q}_i^T \mathbf{x} + r_i \leq 0$,
    i.e., $\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2} \leq 0$.
    \par
    Then (128) can be formulated as
    \begin{align}
        &\parallel A_i'\mathbf{x} + \mathbf{b}_i \parallel^2
        \leq (\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})^2 \\
        \Leftrightarrow \ & \parallel A_i'\mathbf{x} + \mathbf{b}_i \parallel
        \leq -(\mathbf{q}_i^T \mathbf{x} + r_i - \frac{1}{2})
    \end{align}
\end{proof}

\section{The Lagrangian}